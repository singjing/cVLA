{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "return_depth = False\n",
    "\n",
    "#dataset_location = \"/tmp/clevr-act-6-var-cam2\"\n",
    "dataset_location = \"/data/lmbraid19/argusm/datasets/clevr-real-block-simple-v3\"\n",
    "#dataset_location = \"/data/lmbraid19/argusm/datasets/clevr-real-1of5c-v1\"\n",
    "dataset_location = Path(dataset_location)\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models/\")\n",
    "#model_path = model_location / \"clevr-act-7-depth_l40\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-6-var-cam2_hf_af_lr3e5\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_rbg20\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_text_aug\" / \"checkpoint-4687\"\n",
    "model_path = model_location / \"clevr-act-7-depth_depthaug\" / \"checkpoint-4687\"\n",
    "\n",
    "\n",
    "#model_path = model_location / \"clevr-act-7-depth_depth_l40\" / \"checkpoint-4687\"\n",
    "if \"depth_depth\" in str(model_path):\n",
    "    return_depth = True\n",
    "\n",
    "print(\"dataset_location\", dataset_location)\n",
    "if model_path.is_dir():\n",
    "    print(\"moadel_path is\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader_jsonl import JSONLDataset, ValidDataset\n",
    "\n",
    "test_dataset = JSONLDataset(\n",
    "    jsonl_file_path=f\"{dataset_location}/_annotations.valid.jsonl\",\n",
    "    image_directory_path=f\"{dataset_location}/dataset\",\n",
    "    clean_prompt=True,\n",
    "    return_depth=return_depth\n",
    ")\n",
    "\n",
    "print(\"dataset len\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from utils_vis import render_example\n",
    "\n",
    "def get_image(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[-1]\n",
    "    else:\n",
    "        return images\n",
    "    \n",
    "def get_depth(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(len(test_dataset))\n",
    "num_samples = min(3*1, len(test_dataset))\n",
    "html_imgs = \"\"\n",
    "for i in tqdm(range(num_samples)):\n",
    "    images, sample = test_dataset[i]\n",
    "    image = get_depth(images) if return_depth else get_image(images)\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], text=sample[\"prefix\"], camera=sample[\"camera\"])\n",
    "    \n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"loaded processor.\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_path, torch_dtype=TORCH_DTYPE, device_map=\"auto\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_depth:\n",
    "    def collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\n",
    "        #suffixes = [label[\"suffix\"] for label in labels]\n",
    "        images_flat = [img for img_list_x in images for img in img_list_x]\n",
    "        inputs = processor(\n",
    "            text=prefixes,\n",
    "            images=images_flat,\n",
    "            return_tensors=\"pt\",\n",
    "            #suffix=suffixes,\n",
    "            padding=\"longest\"\n",
    "        ).to(TORCH_DTYPE).to(DEVICE)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from utils_vis import render_example\n",
    "from utils_trajectory import DummyCamera\n",
    "from math import ceil\n",
    "\n",
    "eval_batch_size = 8\n",
    "#if return_depth:\n",
    "#    eval_batch_size = 1\n",
    "\n",
    "#def augment_sample(sample):\n",
    "#    sample[1][\"prefix\"] = sample[1][\"prefix\"].replace(\"blue cup\", \"crockery\").replace(\"cup\",\"crockery\")\n",
    "    \n",
    "#test_samples = eval_batch_size*3\n",
    "test_samples = len(test_dataset)\n",
    "pred_list = []\n",
    "html_imgs = \"\"\n",
    "for start_idx in tqdm(range(0, test_samples, eval_batch_size), total=ceil(test_samples / eval_batch_size)):\n",
    "    batch = [test_dataset[i] for i in range(start_idx, min(start_idx + eval_batch_size, test_samples))]\n",
    "    #for e in batch:\n",
    "    #    augment_sample(e)\n",
    "    inputs = collate_fn(batch)\n",
    "    prefix_length = inputs[\"input_ids\"].shape[-1]    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "        decoded = [processor.decode(x, skip_special_tokens=True) for x in generation[:, prefix_length:]]\n",
    "    pred_list.extend(decoded)\n",
    "    \n",
    "    for batch_entry, decoded_str in zip(batch, decoded):\n",
    "        if return_depth:\n",
    "            (depth, image), sample = batch_entry\n",
    "        else:\n",
    "            image, sample = batch_entry\n",
    "        html_img = render_example(image, text=sample[\"prefix\"], label=sample[\"suffix\"], prediction=decoded_str, camera=sample[\"camera\"])\n",
    "        html_imgs += html_img\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = []\n",
    "for i, pred in enumerate(pred_list):\n",
    "    suffix = test_dataset[i][1][\"suffix\"]\n",
    "    suffix_p = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", suffix)]\n",
    "    decode_p = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", pred)]\n",
    "    if len(decode_p) != 12:\n",
    "        continue\n",
    "    pred_diff = np.array(suffix_p) - np.array(decode_p)\n",
    "    results.append(pred_diff)\n",
    "results = np.array(results)\n",
    "\n",
    "results[:, 0:2] = results[:, 0:2]/1024*224 \n",
    "results[:, 6:8] = results[:, 6:8]/1024*224 \n",
    "\n",
    "\n",
    "keypoint = [\"object\",\"container\"]\n",
    "action_labels = [\"x\",\"y\",\"depth\",\"r1\",\"r2\",\"r3\"]*2\n",
    "units         = [\"px\",\"px\",\"cm\",\"rad\",\"rad\",\"rad\"]*2\n",
    "plot_hist=True\n",
    "if plot_hist:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 12*2/3))  # 3 rows x 4 columns of histograms\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "    for i in range(2):\n",
    "        for j in range(6):\n",
    "            axes[j].hist(results[:, i*6+j], bins=20, alpha=0.7,  edgecolor='black', label=keypoint[i])\n",
    "            axes[j].set_title(f'Hist. {action_labels[j]} err.')\n",
    "            axes[j].set_xlabel(f\"{action_labels[j]} err. [{units[j]}]\")\n",
    "            axes[j].set_ylabel('Frequency')\n",
    "            axes[j].legend()\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()\n",
    "\n",
    "print(\"Valid Samples:\", len(results), \"L1:\",np.mean(np.abs(results)))\n",
    "print()\n",
    "print(\"L1_depth\", np.mean(np.abs(results[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB Networks\n",
    "# Valid Samples: 157 L1: 35.67 (no-agug)\n",
    "# Valid Samples: 160 L1: 34.35 (prompt cleaning) 33.89\n",
    "# Valid Samples: 160 L1: 34.62 (prompt cleaning + simplify text)  -- similar\n",
    "\n",
    "# Valid Samples: 160 L1: 24.96 (rgb20 = augmentation + random background (CVPR09-dataset))\n",
    "# Valid Samples: 160 L1: 26.61 (rgb20 + simplify text) -- worse\n",
    "# Valid Samples: 150 L1: 40.55 (augmentation + DROID background)  -- way worse\n",
    "\n",
    "\n",
    "# With depth (only 31 samples for real data)\n",
    "# Valid Samples: 31 L1: 29.24 (no text aug, NaN-to-Max eval)\n",
    "# Valid Samples: 31 L1: 28.58  L1_depth 6.29375 (no text aug, Nan-to-Min eval)  \n",
    "# Valid Samples: 160 L1: 27.93 6.29375 (no text aug, all-zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_on_blocks = {\"baseline\":35.67, \"+clean prompt\": 33.89, \"(simplify text)\":34.62,\n",
    "                  \"rbg-20%\": 24.96, \"(droid bg)\": 40.55, \"text aug\": 25.93}\n",
    "\n",
    "\n",
    "# Extract labels and values\n",
    "labels, values = zip(*eval_on_blocks.items())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Formatting\n",
    "plt.ylabel(\"L1 Error Mean\")\n",
    "plt.title(\"Evaluation over Training Data\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "plt.xlabel(\"Experiment Runs\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  # Add grid lines for readability\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import re\n",
    "# import numpy as np\n",
    "# results = []\n",
    "# suffix_nums = []\n",
    "# predi_nums = []\n",
    "# for i in range(test_samples):\n",
    "#     suffix = test_dataset[i][1][\"suffix\"]\n",
    "#     prefix = test_dataset[i][1][\"prefix\"]\n",
    "#     decoded = decode_dataset[i]\n",
    "#     suffix_p = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", suffix)]\n",
    "#     try:\n",
    "#         decode_p = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", decoded)]\n",
    "#     except ValueError:\n",
    "#         continue\n",
    "#     if len(decode_p) != 12:\n",
    "#         continue\n",
    "#     pred_diff = np.array(suffix_p) - np.array(decode_p)\n",
    "#     results.append(pred_diff)\n",
    "#     suffix_nums.append(suffix_p)\n",
    "#     predi_nums.append(decode_p)\n",
    "\n",
    "# results = np.array(results)\n",
    "# #suffix_nums = np.array(suffix_nums) / 1024 * 448\n",
    "# #predi_nums = np.array(predi_nums) / 1024 * 448\n",
    "\n",
    "# print(results)\n",
    "# print(np.abs(results).mean(axis=0).round())\n",
    "\n",
    "# plot_histogram = True\n",
    "# if plot_histogram:\n",
    "#     fig, axes = plt.subplots(4, 3, figsize=(10, 12))  # 3 rows x 4 columns of histograms\n",
    "#     axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "#     for i in range(12):\n",
    "#         axes[i].hist(results[:, i], bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "#         axes[i].set_title(f'Histogram for Column {i + 1}')\n",
    "#         axes[i].set_xlabel('Value')\n",
    "#         axes[i].set_ylabel('Frequency')\n",
    "\n",
    "#     plt.tight_layout()  # Adjust layout for better spacing\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# class ModelWrapper:\n",
    "#     def __init__(self, transformers_model=model):\n",
    "#         self.model = transformers_model\n",
    "    \n",
    "#     def make_predictions(self, image, prefix):\n",
    "#         prefix = \"<image>\" + prefix\n",
    "#         image = Image.fromarray(image)\n",
    "#         inputs = processor(text=prefix,\n",
    "#                            images=image,\n",
    "#                            return_tensors=\"pt\").to(TORCH_DTYPE).to(DEVICE)\n",
    "#         prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "#         with torch.inference_mode():\n",
    "#             generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "#             generation = generation[0][prefix_length:]\n",
    "#             decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "#         return None, None, None, decoded\n",
    "# model_wrapped = ModelWrapper(model)\n",
    "\n",
    "# i = 0\n",
    "# image, label = test_dataset[i]\n",
    "# print(image)\n",
    "# print(label[\"prefix\"])\n",
    "# res = model_wrapped.make_predictions(np.asarray(image), label[\"prefix\"])\n",
    "# print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# import json\n",
    "# from PIL import Image\n",
    "# from mani_skill.examples.run_env import Args, iterate_env, save_dataset\n",
    "\n",
    "        \n",
    "# parsed_args = Args()\n",
    "# parsed_args.env_id = \"ClevrMove-v1\"\n",
    "# parsed_args.render_mode = \"rgb_array\"\n",
    "# parsed_args.control_mode = \"pd_joint_pos\"\n",
    "\n",
    "# env_iter = iterate_env(parsed_args, vis=False, model=model_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(25):\n",
    "#     next(env_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model_path = model_location / \"clevr-act-6-var-cam2_hf_af_lr3e5\"\n",
    "# training_args_good = torch.load(model_path / \"checkpoint-4687\" / \"training_args.bin\", weights_only=False)\n",
    "\n",
    "# model_path = model_location / \"clevr-act-7-depth_test_h5\"\n",
    "# training_args_bad = torch.load(model_path / \"checkpoint-start\" / \"training_args.bin\", weights_only=False)\n",
    "\n",
    "# def compare(a, b, depth=0):\n",
    "#     for attr in dir(a):\n",
    "#         if attr.startswith(\"__\"):\n",
    "#             continue\n",
    "#         a_good = a.__getattribute__(attr)\n",
    "#         a_bad = a.__getattribute__(attr)\n",
    "\n",
    "#         if a_good == a_bad:\n",
    "#             continue\n",
    "#         elif isisntance(a_good, (str, int, list, tuple)):\n",
    "#             print(attr, \"good=\",a_good, \"bad=\",a_bad)\n",
    "#         elif depth < 2:\n",
    "#             print(type(a_good))\n",
    "#             compare(a_good, a_bad, depth+1)\n",
    "        \n",
    "        \n",
    "            \n",
    "# compare(training_args_good, training_args_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
