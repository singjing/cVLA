{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "dataset_location = Path(\"/tmp/clevr-act-6-var-cam\")\n",
    "\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "model_path = Path(str(model_location / Path(dataset_location).stem) + \"_hf_af_lr5e5\")/ \"checkpoint-4687\"\n",
    "print(model_path)\n",
    "assert(model_path.is_dir())\n",
    "\n",
    "print_head = lambda file_path, n=1: print(\"\\n\".join([line.strip() for _, line in zip(range(n), open(file_path))]))\n",
    "\n",
    "print(\"dataset_location\", dataset_location)\n",
    "print()\n",
    "print_head(dataset_location / \"dataset\" /\"_annotations.train.jsonl\", 5)\n",
    "print_head(dataset_location / \"info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from data_loader import JSONLDataset\n",
    "\n",
    "train_dataset = JSONLDataset(\n",
    "    jsonl_file_path=f\"{dataset_location}/dataset/_annotations.train.jsonl\",\n",
    "    image_directory_path=f\"{dataset_location}/dataset\",\n",
    ")\n",
    "valid_dataset = JSONLDataset(\n",
    "    jsonl_file_path=f\"{dataset_location}/dataset/_annotations.valid.jsonl\",\n",
    "    image_directory_path=f\"{dataset_location}/dataset\",\n",
    ")\n",
    "test_dataset = JSONLDataset(\n",
    "    jsonl_file_path=f\"{dataset_location}/dataset/_annotations.valid.jsonl\",\n",
    "    image_directory_path=f\"{dataset_location}/dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "\n",
    "    paths = [label[\"image\"] for label in labels]\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_path, torch_dtype=TORCH_DTYPE, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "test_samples = 25\n",
    "decode_dataset = [None, ]*test_samples\n",
    "for i in tqdm(range(test_samples), total=test_samples):\n",
    "    image, label = test_dataset[i]\n",
    "    prefix = \"<image>\" + label[\"prefix\"]\n",
    "    suffix = label[\"suffix\"]\n",
    "    inputs = processor(\n",
    "        text=prefix,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix = [augment_suffix(suffix)]\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "    prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "        generation = generation[0][prefix_length:]\n",
    "        decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    decode_dataset[i] = decoded\n",
    "\n",
    "print(decode_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "results = []\n",
    "suffix_nums = []\n",
    "predi_nums = []\n",
    "for i in range(test_samples):\n",
    "    suffix = test_dataset[i][1][\"suffix\"]\n",
    "    prefix = test_dataset[i][1][\"prefix\"]\n",
    "    decoded = decode_dataset[i]\n",
    "    #print(\"prefix:\", prefix)\n",
    "    #print(suffix)\n",
    "    #print(decoded)\n",
    "    #print()\n",
    "    suffix_p = [int(x) for x in re.findall(r\"<loc(\\d{4})>\", suffix)]\n",
    "    try:\n",
    "        decode_p = [int(x) for x in re.findall(r\"<loc(\\d{4})>\", decoded)]\n",
    "    except ValueError:\n",
    "        continue\n",
    "    if len(decode_p) != 12:\n",
    "        continue\n",
    "    pred_diff = np.array(suffix_p) - np.array(decode_p)\n",
    "    results.append(pred_diff)\n",
    "    suffix_nums.append(suffix_p)\n",
    "    predi_nums.append(decode_p)\n",
    "results = np.array(results)\n",
    "suffix_nums = np.array(suffix_nums) / 1024 * 448\n",
    "predi_nums = np.array(predi_nums) / 1024 * 448\n",
    "\n",
    "print(results)\n",
    "print(np.abs(results).mean(axis=0).round())\n",
    "\n",
    "plot_histogram = False\n",
    "if plot_histogram:\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(10, 12))  # 3 rows x 4 columns of histograms\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "    for i in range(12):\n",
    "        axes[i].hist(results[:, i], bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[i].set_title(f'Histogram for Column {i + 1}')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better spacing\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from utils_traj_tokens import decode_caption_xyzrotvec\n",
    "from utils_trajectory import DummyCamera\n",
    "\n",
    "def read_n_lines(file_path: str, n: int) -> List[str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [next(file).strip() for _ in range(n)]\n",
    "    return lines\n",
    "\n",
    "def plot_tokens(image, tokens, ax,color='green'):\n",
    "    image_height, image_width, _ = image.shape\n",
    "    camera_extrinsic = [[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]]\n",
    "    camera_intrinsic = [[[410.029, 0.0, 224.0], [0.0, 410.029, 224.0], [0.0, 0.0, 1.0]]]\n",
    "    camera = DummyCamera(camera_intrinsic, camera_extrinsic, width=image_width, height=image_height)\n",
    "    curve_25d, _ =  decode_caption_xyzrotvec(tokens, camera)\n",
    "    curve_2d = curve_25d[:, :2]\n",
    "    ax.plot(curve_2d[:, 0], curve_2d[:, 1],'.--', color=color)\n",
    "\n",
    "\n",
    "lines = read_n_lines(f\"{dataset_location}/dataset/_annotations.train.jsonl\", 25)\n",
    "n_rows, n_cols = 5, 5\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "for i, line in enumerate(lines):\n",
    "    ax = axes[i // n_cols][i % n_cols]\n",
    "    data = json.loads(line)\n",
    "    image_path = os.path.join(dataset_location, \"dataset\", data.get('image'))\n",
    "    image = np.asarray(test_dataset[i][0])\n",
    "    ax.imshow(image)\n",
    "    gt = test_dataset[i][1][\"suffix\"]\n",
    "    pd = decode_dataset[i]\n",
    "    line = plot_tokens(image, gt, ax, color='green')\n",
    "    line = plot_tokens(image, pd, ax, color='lime')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ModelWrapper:\n",
    "    def __init__(self, transformers_model=model):\n",
    "        self.model = transformers_model\n",
    "    \n",
    "    def make_predictions(self, image, prefix):\n",
    "        prefix = \"<image>\" + prefix\n",
    "        image = Image.fromarray(image)\n",
    "        inputs = processor(text=prefix,\n",
    "                           images=image,\n",
    "                           return_tensors=\"pt\").to(TORCH_DTYPE).to(DEVICE)\n",
    "        prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "            generation = generation[0][prefix_length:]\n",
    "            decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "        return None, None, None, decoded\n",
    "model_wrapped = ModelWrapper(model)\n",
    "\n",
    "i = 0\n",
    "image, label = test_dataset[i]\n",
    "print(image)\n",
    "print(label[\"prefix\"])\n",
    "res = model_wrapped.make_predictions(np.asarray(image), label[\"prefix\"])\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from PIL import Image\n",
    "from mani_skill.examples.run_env import Args, iterate_env, save_dataset\n",
    "\n",
    "        \n",
    "parsed_args = Args()\n",
    "parsed_args.env_id = \"ClevrMove-v1\"\n",
    "parsed_args.render_mode = \"rgb_array\"\n",
    "parsed_args.control_mode = \"pd_joint_pos\"\n",
    "\n",
    "env_iter = iterate_env(parsed_args, vis=False, model=model_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    next(env_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
