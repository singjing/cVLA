{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from data_loader_h5 import H5Dataset\n",
    "from data_loader_jsonl import JSONLDataset\n",
    "from data_augmentations import RandomizeBackgrounds, augment_image_rgb, complexify_text\n",
    "\n",
    "dataset_location = \"/tmp/clevr-act-7-depth\"\n",
    "#train_dataset = H5Dataset(dataset_location, augment_text=complexify_text)\n",
    "randomize_background = RandomizeBackgrounds(p=0.2, background_images = \"/tmp/indoorCVPR/Images\")\n",
    "train_dataset = H5Dataset(dataset_location, augment_rgbds=randomize_background, augment_rgb=augment_image_rgb, augment_text=complexify_text)\n",
    "\n",
    "\n",
    "\n",
    "#train_dataset = JSONLDataset(jsonl_file_path=\"/data/lmbraid19/argusm/datasets/clevr-real-block-v1\")\n",
    "\n",
    "#dataset_location = Path(\"/tmp/clevr-act-7-depth\")\n",
    "#randomize_background = RandomizeBackgrounds(p=0.2, background_images = \"/tmp/indoorCVPR/Images\")\n",
    "#train_dataset = H5Dataset(dataset_location, augment_rgbds=randomize_background, augment_rgb=augment_image_rgb)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from utils_vis import render_example\n",
    "\n",
    "print(len(train_dataset))\n",
    "num_samples = 25\n",
    "html_imgs = \"\"\n",
    "for i in tqdm(range(num_samples)):\n",
    "    image, sample = train_dataset[i]\n",
    "    html_imgs += render_example(image[1], label=sample[\"suffix\"], text=sample[\"prefix\"], camera=sample[\"camera\"])    \n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"/tmp/clevr-act-7-depth\"\n",
    "train_dataset = H5Dataset(dataset_location, return_depth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import viridis\n",
    "\n",
    "class DepthColorToNorm:\n",
    "    def __init__(self, depth_min=0, depth_max=1023):\n",
    "        viridis_values = np.linspace(0, 1, len(viridis.colors))\n",
    "        # Create LUT dictionary mapping RGB -> value\n",
    "        self.viridis_lut = {tuple(rgb[:3]): value for rgb, value in zip(viridis.colors, viridis_values)}  # Disel: only the brave.\n",
    "        #self.viridis_lut = {rgb_to_key(rgb[:3]): value for rgb, value in zip(viridis.colors, viridis_values)}\n",
    "        self.depth_min = depth_min\n",
    "        self.depth_max = depth_max\n",
    "        \n",
    "    def __call__(self, array):\n",
    "        if isinstance(array, np.ndarray):\n",
    "            old_shape = array.shape\n",
    "            recovered_flat = np.array([self.viridis_lut.get(tuple(color), 0) for color in array.reshape(-1, 3)])\n",
    "            return recovered_flat.reshape(old_shape[:-1])\n",
    "        elif isinstance(array, tuple):\n",
    "            return self.viridis_lut.get(array, 0)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "    # def rgb_to_key(rgb, precision=3):\n",
    "    #         return tuple(np.round(rgb, precision))  # Round for consistent lookup\n",
    "\n",
    "# Step 1: Create a random 4x4 array with values between 0 and 1\n",
    "random_array = np.random.rand(4, 4)\n",
    "viridis_mapped = viridis(random_array)[..., :3]  # Take only RGB, ignore alpha\n",
    "color_to_norm = DepthColorToNorm()\n",
    "recovered_array = color_to_norm(viridis_mapped)\n",
    "np.all(np.isclose(random_array, recovered_array, atol=.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_traj_tokens import decode_caption_xyzrotvec2\n",
    "from tqdm.notebook import tqdm\n",
    "all_tcp_zs = []\n",
    "all_depths = []\n",
    "for i in tqdm(range(200)):\n",
    "    (depth, image), sample = train_dataset[i]\n",
    "    curve_25d, quat_c = decode_caption_xyzrotvec2(sample[\"suffix\"], sample[\"camera\"])\n",
    "    #suffix_int = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", sample[\"suffix\"])]\n",
    "    x, y = curve_25d[0][:2].round().numpy().astype(int)\n",
    "    tcp_z_m = curve_25d[0][2].numpy()\n",
    "\n",
    "    image_depth_color = tuple(depth[y, x])\n",
    "    image_depth_norm = color_to_norm(image_depth_color)\n",
    "    image_depth_m = image_depth_norm * 1023 / 1000\n",
    "\n",
    "    all_tcp_zs.append(tcp_z_m)\n",
    "    all_depths.append(image_depth_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(all_tcp_zs), np.mean(all_depths))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.scatter(all_tcp_zs, all_depths)\n",
    "ax.set_xlabel(\"TCP zs [m]\")\n",
    "ax.set_ylabel(\"image depth [m]\")\n",
    "ax.plot((.2,.8),(.2,.8), color='k')\n",
    "plt.show()\n",
    "\n",
    "bad_idx = np.argmax(np.abs(np.array(all_tcp_zs) - np.array(all_depths)))\n",
    "plt.imshow(train_dataset[bad_idx][0][1])\n",
    "print(train_dataset[bad_idx][1][\"prefix\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 2, 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 12*2/3))  # 3 rows x 4 columns of histograms\n",
    "for c in range(cols):\n",
    "    (depth, image), sample = train_dataset[c]\n",
    "\n",
    "    curve_25d, quat_c = decode_caption_xyzrotvec2(sample[\"suffix\"], sample[\"camera\"])\n",
    "\n",
    "    #suffix_int = [int(x) for x in re.findall(r\"<(?:loc|seg)(\\d+)>\", sample[\"suffix\"])]\n",
    "\n",
    "    x,y = curve_25d[0][:2].round().numpy().astype(int)\n",
    "    depth_val = curve_25d[0][2].numpy()\n",
    "    depth_color = tuple(depth[y, x])\n",
    "    print(depth_color)\n",
    "    depth_norm_float = color_to_norm(depth_color)\n",
    "    depth_float = depth_norm_float * 1023 / 10\n",
    "    \n",
    "    print(depth_float, depth_val)\n",
    "    \n",
    "    d = curve_25d[0][2]\n",
    "    #image[y:y+10,x:x+10] = [255,0,0]\n",
    "\n",
    "    for r in range(rows//2):\n",
    "        axes[r][c].imshow(depth)\n",
    "        axes[r][c].set_title(\"depth\")\n",
    "        axes[r+1][c].imshow(image)\n",
    "        axes[r+1][c].set_title(sample[\"prefix\"].split(\"<\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManiSkill H5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "def print_h5_structure(name, obj):\n",
    "    \"\"\"\n",
    "    Prints the path, type, and shape of each member in an h5 file.\n",
    "    \"\"\"\n",
    "    # Check if the object is a dataset\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"Dataset: {name} | Shape: {obj.shape} | Type: {obj.dtype}\")\n",
    "    # If it's a group, just print its name\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print(f\"Group: {name}\")\n",
    "\n",
    "traj_path = Path(\"/data/lmbraid19/argusm/datasets/clevr-act-9-ms-small/20250205_182607.h5\")\n",
    "idx = 0\n",
    "\n",
    "with h5py.File(traj_path, \"r\") as h5_file:\n",
    "    print(h5_file['traj_0/obs/sensor_param/render_camera'].keys())\n",
    "    print_h5_structure(f\"traj_{idx}\", h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_tree(val, pre=''):\n",
    "    items = len(val)\n",
    "    for key, val in val.items():\n",
    "        items -= 1\n",
    "        if items == 0:\n",
    "            # the last item\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '└── ' + key)\n",
    "                h5_tree(val, pre+'    ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '└── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '└── ' + key + ' (scalar)')\n",
    "        else:\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '├── ' + key)\n",
    "                h5_tree(val, pre+'│   ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '├── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '├── ' + key + ' (scalar)')\n",
    "\n",
    "h5_tree(train_dataset.h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
