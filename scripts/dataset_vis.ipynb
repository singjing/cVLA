{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r /tmp/cvla-7-obja-small\n",
    "#%cd /ihome/argusm/lang/ManiSkill/mani_skill/examples\n",
    "#!python run_env.py --record_dir /tmp/cvla-7-obja-small \n",
    "#!python run_env.py -od objaverse --record_dir /tmp/cvla-7-obja --N_samples 50000\n",
    "#!rsync -a --progress /data/lmbraid19/argusm/datasets/clevr-act-7-depth /tmp/\n",
    "#!ls /tmp/clevr-act-7-depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_loader_jsonl import JSONLDataset\n",
    "from cvla.data_loader_images import ImageFolderDataset\n",
    "from cvla.data_augmentations import RandomizeBackgrounds, augment_image_rgb, complexify_text, DepthAugmentation\n",
    "\n",
    "return_depth = False\n",
    "depth_to_color = False\n",
    "action_encoder = \"xyzrotvec-cam-512xy128d\"\n",
    "depth_augment = DepthAugmentation()\n",
    "depth_augment = None\n",
    "\n",
    "#dataset_location = \"/data/lmbraid19/argusm/datasets/cvla-test\"\n",
    "#dataset_location = \"/data/lmbraid19/argusm/datasets/clevr-act-7-small\"\n",
    "\n",
    "dataset_location = \"/tmp/cvla-clevr-8\"\n",
    "#dataset_location = \"/tmp/cvla-obja-camRF-sceneR-9\"\n",
    "\n",
    "dataset = H5Dataset(dataset_location, return_depth=return_depth, augment_depth=depth_augment, depth_to_color=depth_to_color, action_encoder=action_encoder,\n",
    "                    return_robot_pose=True)\n",
    "enc_data = dataset.action_encoder\n",
    "\n",
    "# dataset_location = \"/tmp/cvla-7-obja\"\n",
    "# dataset_location2 = \"/tmp/clevr-act-7-depth\"\n",
    "# from torch.utils.data import ConcatDataset\n",
    "#dataset1 = H5Dataset(dataset_location, return_depth=return_depth, augment_depth=depth_augment, depth_to_color=depth_to_color, action_encoder=action_encoder, limit_samples=50_000)\n",
    "#dataset2 = H5Dataset(dataset_location2, return_depth=return_depth, augment_depth=depth_augment, depth_to_color=depth_to_color, action_encoder=action_encoder, limit_samples=30)\n",
    "#decc_data = dataset1.action_encoder.decode_caption\n",
    "#dataset = ConcatDataset([dataset1, dataset2])\n",
    "print(\"dataset len\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from cvla.utils_vis import render_example\n",
    "\n",
    "def get_image(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[-1]\n",
    "    else:\n",
    "        return images\n",
    "    \n",
    "def get_depth(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(\"Real Samples:\", len(dataset))\n",
    "num_samples = min(3*20, len(dataset))\n",
    "html_imgs = \"\"\n",
    "for i in tqdm(range(num_samples)):\n",
    "    images, sample = dataset[i]\n",
    "    image = get_image(images)\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], text=f\"{i} \"+sample[\"prefix\"], camera=sample[\"camera\"], enc=enc_data)\n",
    "    \n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Depth Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cvla.data_augmentations import depth_to_norm, norm_to_color\n",
    "from PIL import Image\n",
    "\n",
    "images, sample = dataset[0]\n",
    "if depth_to_color:\n",
    "    real_depth_image = get_depth(images)\n",
    "    real_depth_image = (real_depth_image*255).round().astype(np.uint8)\n",
    "    real_depth_image = Image.fromarray(real_depth_image).resize((448,448))\n",
    "    \n",
    "images, sample = dataset[2]\n",
    "sim_depth_image = get_depth(images)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8, 8*1/2))\n",
    "ax[0].imshow(np.clip(real_depth_image, 0, 1023))\n",
    "ax[1].imshow(np.clip(sim_depth_image, 0,1023))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def get_non_zero_min(arr):\n",
    "    return arr[arr!=0].min()\n",
    "\n",
    "real_mins = []\n",
    "sim_mins = []\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    images, sample = dataset[i]\n",
    "    real_depth_image = get_depth(images)\n",
    "    real_mins.append(get_non_zero_min(real_depth_image))\n",
    "    images, sample = dataset[i]\n",
    "    sim_depth_image = get_depth(images)\n",
    "    sim_mins.append(get_non_zero_min(sim_depth_image))\n",
    "\n",
    "plt.plot(sorted(real_mins), label=\"real\")\n",
    "plt.plot(sorted(sim_mins), label=\"sim\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "inspect.getfile(render_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from cvla.utils_traj_tokens import decode_caption_xyzrotvec2\n",
    "from cvla.data_augmentations import  color_to_norm, norm_to_depth\n",
    "\n",
    "results = {}\n",
    "for name, dataset in zip((\"real\", \"sim\"), (dataset, dataset)):\n",
    "    num_samples = min(len(dataset), 100)\n",
    "    all_tcp_zs = []\n",
    "    all_depths = []\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        (depth, image), sample = dataset[i]\n",
    "        curve_25d, quat_c = decode_caption_xyzrotvec2(sample[\"suffix\"], sample[\"camera\"])\n",
    "        x, y = curve_25d[0][:2].round().numpy().astype(int)\n",
    "        tcp_z_m = curve_25d[0][2].numpy()\n",
    "        if depth.ndim == 3:\n",
    "            image_depth_color = tuple(depth[y, x])\n",
    "            image_depth_norm = color_to_norm(image_depth_color)\n",
    "            image_depth_m = norm_to_depth(image_depth_norm) / 1000\n",
    "        else:\n",
    "            image_depth_m = depth[y, x] / 1000\n",
    "        if image_depth_m == 0:\n",
    "            continue\n",
    "        all_tcp_zs.append(tcp_z_m)\n",
    "        all_depths.append(image_depth_m)\n",
    "    results[name] = dict(tcp_zs=np.array(all_tcp_zs), depths=np.array(all_depths))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "for name, res in results.items():\n",
    "    m ,b = np.polyfit(res[\"tcp_zs\"], res[\"depths\"], 1)\n",
    "    print(f\"{name}\\t y={m:.4f} x + {b:.4f}\")\n",
    "    s = 100\n",
    "    if name == \"sim\":\n",
    "        offset_m = 0.05\n",
    "        offset_m = 0\n",
    "        ax.scatter(res[\"tcp_zs\"]*s,(res[\"depths\"]+offset_m)*s, label=name, alpha=.7)\n",
    "    else:\n",
    "        ax.scatter(res[\"tcp_zs\"]*s,res[\"depths\"]*s, label=name, alpha=.7)\n",
    "    ax.set_xlabel(\"TCP zs [cm]\")\n",
    "    ax.set_ylabel(\"image depth [cm]\")\n",
    "\n",
    "    ax.plot((.3*s,.8*s),(.3*s,.8*s), color='k')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows, cols = 2, 2\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 12*2/3))  # 3 rows x 4 columns of histograms\n",
    "for c in range(cols):\n",
    "    (depth, image), sample = dataset[c]\n",
    "    curve_25d, quat_c = decode_caption_xyzrotvec2(sample[\"suffix\"], sample[\"camera\"])\n",
    "    x,y = curve_25d[0][:2].round().numpy().astype(int)\n",
    "    depth_val = curve_25d[0][2].numpy()\n",
    "\n",
    "    if depth.ndim == 3:\n",
    "        depth_color = tuple(depth[y, x])\n",
    "        depth_norm_float = color_to_norm(depth_color)\n",
    "        depth_float = norm_to_depth(depth_norm_float)\n",
    "    else:\n",
    "        depth_float = depth[y, x] * 1023 / 10\n",
    "    \n",
    "    print(depth_float, depth_val)\n",
    "    \n",
    "    d = curve_25d[0][2]\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        image = np.array(image)\n",
    "\n",
    "    for r in range(rows//2):\n",
    "        axes[r][c].imshow(depth)\n",
    "        circle = plt.Circle((x, y), 20, color='magenta', fill=False)\n",
    "        axes[r][c].add_patch(circle)  # Add circle to the image subplot\n",
    "\n",
    "        axes[r][c].set_title(\"depth\")\n",
    "        axes[r+1][c].imshow(image)\n",
    "        axes[r+1][c].set_title(sample[\"prefix\"].split(\"<\")[0])\n",
    "        circle = plt.Circle((x, y), 20, color='magenta', fill=False)\n",
    "        axes[r+1][c].add_patch(circle)  # Add circle to the image subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "texts = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    images, sample = dataset[i]\n",
    "    text = sample[\"prefix\"].split(\"<\")[0]\n",
    "    texts.append(text)\n",
    "    \n",
    "# Sort texts by their length\n",
    "texts_sorted = sorted(texts, key=len)\n",
    "\n",
    "# Print sorted texts (optional: limit number printed)\n",
    "for t in texts_sorted[::-1][:10]:  # change 20 to see more/less\n",
    "    print(f\"{len(t):3d}: {t}\")\n",
    "\n",
    "text_lengths = [len(t) for t in texts_sorted]\n",
    "\n",
    "p95 = np.percentile(text_lengths, 98)\n",
    "print(f\"95th percentile text length: {int(p95)}\")\n",
    "\n",
    "# Plot histogram of text lengths\n",
    "plt.hist(text_lengths, bins=30)\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Text Lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check to see if robot pose is constant\n",
    "#for i in range(10):\n",
    "#    print(dataset[100+i][1][\"robot_pose\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plane Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install open3d\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "depth_augment = DepthAugmentation()\n",
    "\n",
    "dataset = JSONLDataset(dataset_location, return_depth=return_depth, augment_depth=depth_augment, depth_to_color=False)\n",
    "sample = dataset[10]\n",
    "rgb_image = sample[0][1]\n",
    "depth_image = sample[0][0]\n",
    "width, height = rgb_image.size\n",
    "intrinsic = sample[1][\"camera\"].get_intrinsic_matrix()[0].numpy()\n",
    "camera = o3d.camera.PinholeCameraIntrinsic(width, height, intrinsic)\n",
    "\n",
    "color_as_img = o3d.geometry.Image(np.asarray(rgb_image))\n",
    "depth_as_img = o3d.geometry.Image((depth_image))\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_as_img, depth_as_img)\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, camera)\n",
    "\n",
    "nb_neighbors=20\n",
    "std_ratio=2.0\n",
    "cl, ind = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)\n",
    "\n",
    "\n",
    "pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "o3d.visualization.draw_geometries([pcd], zoom=0.5)\n",
    "print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManiSkill H5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "#dataset_location2 = \"/data/lmbraid19/argusm/datasets/clevr-act-7-small\"\n",
    "#dataset = H5Dataset(dataset_location2, limit_samples=30)\n",
    "\n",
    "\n",
    "#print(dataset.h5_file['traj_0/obs/sensor_data/render_camera/'].keys())\n",
    "#print(dataset.h5_file['traj_0/sensor_data/render_camera/segmentation'][0])\n",
    "#image = dataset.h5_file['traj_0/obs/sensor_data/render_camera/segmentation'][0]\n",
    "#print(np.unique(image))\n",
    "\n",
    "\n",
    "#plt.imshow(image)\n",
    "#for i,(k,v) in enumerate(dataset.h5_file.items()):\n",
    "#    print(k)\n",
    "#    if i > 20:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from pathlib import Path\n",
    "# import h5py\n",
    "\n",
    "# def print_h5_structure(name, obj):\n",
    "#     \"\"\"\n",
    "#     Prints the path, type, and shape of each member in an h5 file.\n",
    "#     \"\"\"\n",
    "#     # Check if the object is a dataset\n",
    "#     if isinstance(obj, h5py.Dataset):\n",
    "#         print(f\"Dataset: {name} | Shape: {obj.shape} | Type: {obj.dtype}\")\n",
    "#     # If it's a group, just print its name\n",
    "#     elif isinstance(obj, h5py.Group):\n",
    "#         print(f\"Group: {name}\")\n",
    "\n",
    "# traj_path = Path(\"/data/lmbraid19/argusm/datasets/clevr-act-9-ms-small/20250205_182607.h5\")\n",
    "# idx = 0\n",
    "\n",
    "# with h5py.File(traj_path, \"r\") as h5_file:\n",
    "#     print(h5_file['traj_0/obs/sensor_param/render_camera'].keys())\n",
    "#     print_h5_structure(f\"traj_{idx}\", h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def h5_tree(val, pre=''):\n",
    "    items = len(val)\n",
    "    for key, val in val.items():\n",
    "        items -= 1\n",
    "        if items == 0:\n",
    "            # the last item\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '└── ' + key)\n",
    "                h5_tree(val, pre+'    ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '└── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '└── ' + key + ' (scalar)')\n",
    "        else:\n",
    "            if type(val) == h5py._hl.group.Group:\n",
    "                print(pre + '├── ' + key)\n",
    "                h5_tree(val, pre+'│   ')\n",
    "            else:\n",
    "                try:\n",
    "                    print(pre + '├── ' + key + ' (%d)' % len(val))\n",
    "                except TypeError:\n",
    "                    print(pre + '├── ' + key + ' (scalar)')\n",
    "\n",
    "\n",
    "#dataset_location = \"/tmp/cvla-test\"\n",
    "dataset = H5Dataset(dataset_location, return_depth=return_depth, augment_depth=depth_augment, depth_to_color=depth_to_color, action_encoder=action_encoder)\n",
    "h5_tree(dataset.h5_file['traj_3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "raw_data = dataset.h5_file['traj_3/obs_scene'][()]\n",
    "decoded = json.loads(raw_data.decode(\"utf-8\"))\n",
    "print(decoded[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
