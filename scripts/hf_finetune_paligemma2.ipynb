{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LqvmtZPzyY1"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Fine-tune PaliGemma2 on Object Detection Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md)\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)\n",
    "\n",
    "PaliGemma 2 is built by combining the SigLIP-So400m vision encoder with the more recent and capable language models from the Gemma 2 family.\n",
    "\n",
    "![PaliGemma2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-1.png)\n",
    "\n",
    "The authors use a 3-stage training approach similar to the original PaliGemma. In stage 1, they combine the pretrained vision and language model components and train them jointly on a multimodal task mixture. In stage 2, they train the models at higher resolutions of 448px^2 and 896px^2. In stage 3, they fine-tune the models on the target transfer tasks.\n",
    "\n",
    "PaliGemma 2 models outperform the original PaliGemma at the same resolution and model size. Increasing the model size and resolution generally improves performance across a wide range of tasks, but the benefits differ depending on the task. Some tasks benefit more from increased resolution, while others benefit more from a larger language model.\n",
    "\n",
    "![PaliGemma2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-2.png)\n",
    "\n",
    "Notebook requires A100 with 40GB of VRAM to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBp3Czz3GBmc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADTkh-2y_9Yv"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To fine-tune PaliGemma2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
    "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
    "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
    "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wyojKiG_hX9"
   },
   "source": [
    "### Select the runtime\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_8BLW6R_x-z",
    "outputId": "678f6080-89ea-41f7-eecb-a592d1a03d82"
   },
   "outputs": [],
   "source": [
    "!(nvidia-smi |tr -s ' '|grep -Eo \"| [0123456789]+ N/A N/A [0-9]{3,} .*\"|awk -F' ' '{system(\"s=$(cat /proc/\"$4\"/cmdline| tr \\\"\\\\0\\\" \\\" \\\");u=$(ps -o uname= -p \"$4\");echo \"$1\"sep\"$4\"sep$u sep\"$7\"sep\" ) }'|sed 's/sep/\\t/g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMlw3ru1YvLg"
   },
   "source": [
    "### Download dataset from Roboflow Universe\n",
    "\n",
    "To fine-tune PaliGemma2, prepare your dataset in JSONL format. You can use Roboflow to easily convert any dataset into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtvz4QZ9YuG8",
    "outputId": "8edb179f-f6a1-421d-f50a-2ee61724415a"
   },
   "outputs": [],
   "source": [
    "#%pip install transformers==4.48.0 accelerate==1.3.0 (this is what I currently use)\n",
    "#%pip install -q peft bitsandbytes transformers==4.47.0 tf-keras\n",
    "#%pip install git+https://github.com/huggingface/transformers\n",
    "#%pip install git+https://github.com/huggingface/accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ElnTKvBVa"
   },
   "source": [
    "**NOTE:** Let's read the first few lines of the annotation file and examine the dataset format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McU9159EvkeA"
   },
   "source": [
    "### Set up and test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "19pQQjIixL-T",
    "outputId": "c59a8054-4f92-4cb0-b1cb-6a7b89e6e4b6"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "\n",
    "from cvla.utils_vis import render_example\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_loader_images import ImageFolderDataset\n",
    "from cvla.data_augmentations import RandomizeBackgrounds, augment_image_rgb, complexify_text, DepthAugmentation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "#dataset_location = Path(\"/tmp/cvla-7-obja\")\n",
    "#dataset_location = Path(\"/tmp/clevr-act-7-depth\")\n",
    "dataset_location = \"/tmp/cvla-obja-camRF-sceneR-9\"\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "save_path = model_location / (str(Path(dataset_location).stem) + \"_e512s_depth\")\n",
    "\n",
    "return_depth = True\n",
    "action_encoder = \"xyzrotvec-cam-512xy128d\"\n",
    "bg_image_dataset = ImageFolderDataset(\"/tmp/indoorCVPR/Images\", transform=transforms.RandomResizedCrop((448, 448)))\n",
    "randomize_background = RandomizeBackgrounds(p=0.2, background_images = bg_image_dataset)\n",
    "#augment_depth = DepthAugmentation(depth_range=(25, 100), max_delta_depth=30)\n",
    "train_dataset = H5Dataset(dataset_location, augment_rgbds=randomize_background, augment_rgb=augment_image_rgb, augment_text=complexify_text,\n",
    "                          action_encoder=action_encoder, return_depth=return_depth)\n",
    "decc_data = train_dataset.action_encoder.decode_caption\n",
    "\n",
    "print(\"dataset_location\", dataset_location)\n",
    "print(\"save_path\", save_path)\n",
    "\n",
    "cur_path = Path(\"hf_finetune_paligemma2.ipynb\").resolve()\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "shutil.copy(cur_path, save_path / (\"train_\" + str(cur_path.name)))\n",
    "json.dump(dict(return_depth=return_depth, action_encoder=action_encoder), open(save_path / \"cvla_info.json\",\"w\"))\n",
    "\n",
    "num_samples = 3*4\n",
    "html_imgs = \"\"\n",
    "for i in range(num_samples):\n",
    "    image, sample = train_dataset[i]\n",
    "    image = image[1] if len(image) > 1 else image\n",
    "    prefix = sample[\"prefix\"]\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], text=prefix, camera=sample[\"camera\"], enc=train_dataset.action_encoder)\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvYNxYbBtE3"
   },
   "source": [
    "### Load PaliGemma2 model\n",
    "\n",
    "**NOTE:** PaliGemma2 offers 9 pre-trained models with sizes of `3B`, `10B`, and `28B` parameters, and resolutions of `224`, `448`, and `896` pixels. In this tutorial, I'll be using the [`google/paligemma2-3b-pt-448`](https://huggingface.co/google/paligemma2-3b-pt-448) checkpoint. Resolution has a key impact on the mAP of the trained model, and it seems that `448` offers the most optimal balance between performance and compute resources required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "a5cd856a1fbf413a9aa4edff7cf3e636",
      "23be9405ad7f4cd8817db36f2f3834dd",
      "2e2e11bda8024f97868d45598bff6dc8",
      "11a966b75e124f369844afda5db25ab6",
      "37650cf71172405392ab843fd82360c3",
      "4fc0f4ab93ff4fddb482a8d6820b1253",
      "cda1f28093f04b1ca2e3689d1e314e10",
      "972c99a291d24b9f96ea09df63d5cf4b",
      "83aa606372b848659e13d683fa2fe09e",
      "3c26b58a776346f2920f810d99b21545",
      "fc9c7ae18f7341598a741ee8811896ff",
      "632c3fbb8739429b875687049eb1cf61",
      "7aeec69c06b049738c1fae480103cb1d",
      "c4e5a6e67ec3443fb73bfa86a27df704",
      "9355cb34cea948909504c1144169b693",
      "7a294c9159804eb388a2d7fe67f2920e",
      "7877bed5b8144f78a5a3f8b33741a82b",
      "dcb6f37a97664ed8ad2a699956bd772b",
      "ee2fa3db801344ba8564f700ca5ee2c5",
      "d5b373e9c1fc4d078e5512764235c6ba"
     ]
    },
    "id": "VfCgxIp3EjmC",
    "outputId": "db59a5d0-440f-4ca3-c5a3-194b9e22f7bd"
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2f877f70e9b241f08717bba2b7616198",
      "9393bb98f83e4bf1bd8becdfdfb41f12",
      "41a50a96ca224e0aa12ef142d317e762",
      "cfcdcf908ff04fa19159c98b08c851ce",
      "9165a3ed1f2d4671b089dd00cae0fdbb",
      "6873590d9ac9436facef6cb182e1c077",
      "7ef24d8b5077400b9e005b373ab8d4f1",
      "df5fb754518547b480d95ce715bcdd8b",
      "c3eddb53526e4694bbf5ee3636e1b806",
      "eaf748f0d17148eba7e6351d107c1a50",
      "625fb0f94f5546e0a643192c2bd6c40f",
      "45db0bb8841f48e38c8eb8d28c1e72b8",
      "254a4ab182234215a7aeb6873f6c62af",
      "b7c2a5216879471c982406086571af7e",
      "9db3a6d6cd9548878d0998150f897ff1",
      "3226844ff08a46dcbc982432456d496b",
      "94f8f306b995471291fc610c4f170b26",
      "b91043be62ef4965852d9dcf5faf8edc",
      "141bc95f2dcc485cb9d23f0f01d28f24",
      "a770c3f09c204bf7b879a0d72256977d",
      "545d3d664c8e49cab3270012be39a604",
      "eeb274cb895f416bb85211d8ed07c005",
      "a3be6c18dd6440e48e80a9dc70bf9c9a",
      "f6619c93a7274805bb3005b0898d5f5f",
      "ef7ffed4ac344f7aaf1f39417284f4e3",
      "eab62ec555b146f4802630f6dd38afc6",
      "3dd89974f8b74268bf295119037ab0b8",
      "b18537a50e8f4e4d9829735ea2a98596",
      "d7f77f6a75f9441b8b1231b2eec3123a",
      "9f36c016c2a24d398e84c62964c47277",
      "dce11ec58ce84d9ea0ae3ac26519e796",
      "1a33f188e7ce499ebcb96e7e1617efe4",
      "be391a3e32d04aea837517041123cece",
      "4dd7115fec054f4790524d469bb263f7",
      "b6a7688a29f74e18b9c10ca185585457",
      "ed85f01229c146a48ae86370c36fb588",
      "2e5cf178fc03434faad8539094ddce4a",
      "ae8a2d84dd28458ba68bc94553337064",
      "20f03fcb438749f5a95cd3a72246fafd",
      "6f9622e6bdb1478e99ddddd1e2257f01",
      "f6e321f3c19f41688f3b2fd84243e851",
      "52d311c2b3af4979b433a392e77b5b21",
      "bddb4ea70dab445badbd5a4b2ef66ed4",
      "e9eb6315ff8a42f4911a3cee35e8320b"
     ]
    },
    "id": "ntXj4A3SyEAa",
    "outputId": "898b4343-0089-4ee6-8a54-d1f76d39b59f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "print(\"cuda visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "devices_good = sorted((int(x) for x in os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")))\n",
    "DEVICE = torch.device('cuda')\n",
    "print(DEVICE)\n",
    "print('Using device:', DEVICE)\n",
    "print(\"Good devices\", devices_good)\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE, device_map=\"auto\", attn_implementation='eager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from PIL import Image\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# # Instruct the model to create a caption in Spanish\n",
    "# prompt = \"caption en\"\n",
    "# model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "# input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "#     generation = generation[0][input_len:]\n",
    "#     decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "#     print(decoded)\n",
    "# assert decoded == \"automobile model is a classic car .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)#.to(DEVICE)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "if return_depth:\n",
    "    def collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\n",
    "        suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "        images_flat = [img for img_list_x in images for img in img_list_x]\n",
    "        inputs = processor(\n",
    "            text=prefixes,\n",
    "            images=images_flat,\n",
    "            return_tensors=\"pt\",\n",
    "            suffix=suffixes,\n",
    "            padding=\"longest\"\n",
    "        ).to(TORCH_DTYPE)\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with JAX settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, label_tokens = eval_pred  # Extract predictions and labels\n",
    "#     if isinstance(predictions, tuple):  # Some models return tuples\n",
    "#         predictions = predictions[0]\n",
    "\n",
    "#     # Convert to token indices if necessary (e.g., for text generation models)\n",
    "#     pred_tokens = np.argmax(predictions, axis=-1)  # Assuming logits, take argmax\n",
    "\n",
    "#     pred_texts = processor.tokenizer.batch_decode(pred_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "#     label_text = processor.tokenizer.batch_decode(label_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "\n",
    "#     print(pred_tokens[:,-SEQLEN-1:])\n",
    "#     print(label_tokens[:,-SEQLEN-1:])\n",
    "#     print(label_text)\n",
    "#     print(pred_texts)\n",
    "#     print()\n",
    "#     return {\"accuracy\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        if \"self_attn\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "TRAIN_EXAMPLES = len(train_dataset)\n",
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_DEV = 8\n",
    "if return_depth:\n",
    "    BATCH_SIZE_DEV = 2\n",
    "GRAD_ACCUM = int(round(BATCH_SIZE / BATCH_SIZE_DEV))\n",
    "TRAIN_STEPS = (TRAIN_EXAMPLES // BATCH_SIZE)\n",
    "SEQLEN = 12\n",
    "SAVE_STEPS = int(TRAIN_STEPS / 15)\n",
    "SAVE_LIMIT = 5\n",
    "\n",
    "\n",
    "print(\"TRAIN_STEPS\",TRAIN_STEPS)\n",
    "print(\"GRAD_ACCUM\", GRAD_ACCUM)\n",
    "\n",
    "args_jax = Seq2SeqTrainingArguments(\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE_DEV,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=3e-5,  # 1e-5, 2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=.05,\n",
    "    generation_max_length=SEQLEN,\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    output_dir=save_path,\n",
    "    bf16=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=4,\n",
    "    #dataloader_prefetch_factor=2,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=4,\n",
    "    #per_device_eval_batch_size=BATCH_SIZE_DEV,\n",
    "    #eval_accumulation_steps=GRAD_ACCUM\n",
    ")\n",
    "#gradient_checkpointing=True,\n",
    "#weight_decay=3e-7,\n",
    "#     \n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    args=args_jax,\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys; import torch; import transformers; import tokenizers; import accelerate; \\\n",
    "print('Python Version:', sys.version); \\\n",
    "print('Torch Version:', torch.__version__); \\\n",
    "print('CUDA Available:', torch.cuda.is_available()); \\\n",
    "print('CUDA Device Count:', torch.cuda.device_count()); \\\n",
    "print('GPU Name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'); \\\n",
    "print('Transformers Version:', transformers.__version__); \\\n",
    "print('Tokenizers Version:', tokenizers.__version__); \\\n",
    "print('Accelerate Version:', accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(save_path)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# test_samples = 5\n",
    "# decode_dataset = [None, ]*test_samples\n",
    "# for i in tqdm(range(test_samples), total=test_samples):\n",
    "#     image, label = train_dataset[i]\n",
    "#     prefix = \"<image>\" + label[\"prefix\"]\n",
    "#     suffix = label[\"suffix\"]\n",
    "#     inputs = processor(\n",
    "#         text=prefix,\n",
    "#         images=image,\n",
    "#         return_tensors=\"pt\",\n",
    "#         suffix = [augment_suffix(suffix)]\n",
    "#     ).to(TORCH_DTYPE).to(DEVICE)\n",
    "#     prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "#         generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "#         generation = generation[0][prefix_length:]\n",
    "#         decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "#     print(suffix)\n",
    "#     print(decoded)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference with fine-tuned PaliGemma2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "#model = PaliGemmaForConditionalGeneration.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "test_samples = 25\n",
    "decode_dataset = [None, ]*test_samples\n",
    "for i in tqdm(range(test_samples), total=test_samples):\n",
    "    image, label = test_dataset[i]\n",
    "    prefix = \"<image>\" + label[\"prefix\"]\n",
    "    suffix = label[\"suffix\"]\n",
    "    inputs = processor(\n",
    "        text=prefix,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix = [augment_suffix(suffix)]\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "    prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "        generation = generation[0][prefix_length:]\n",
    "        decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    decode_dataset[i] = decoded\n",
    "\n",
    "print(decode_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ModelWrapper:\n",
    "    def __init__(self, transformers_model=model):\n",
    "        self.model = transformers_model\n",
    "    \n",
    "    def make_predictions(self, image, prefix):\n",
    "        prefix = \"<image>\" + prefix\n",
    "        image = Image.fromarray(image)\n",
    "        inputs = processor(text=prefix,\n",
    "                           images=image,\n",
    "                           return_tensors=\"pt\").to(TORCH_DTYPE).to(DEVICE_0)\n",
    "        prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "            generation = generation[0][prefix_length:]\n",
    "            decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "        return None, None, None, decoded\n",
    "model_wrapped = ModelWrapper(model)\n",
    "\n",
    "i = 0\n",
    "image, label = test_dataset[i]\n",
    "print(image)\n",
    "print(label[\"prefix\"])\n",
    "res = model_wrapped.make_predictions(np.asarray(image), label[\"prefix\"])\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "from PIL import Image\n",
    "from mani_skill.examples.run_env import Args, iterate_env, save_dataset\n",
    "\n",
    "        \n",
    "parsed_args = Args()\n",
    "parsed_args.env_id = \"ClevrMove-v1\"\n",
    "parsed_args.render_mode = \"rgb_array\"\n",
    "parsed_args.control_mode = \"pd_joint_pos\"\n",
    "\n",
    "env_iter = iterate_env(parsed_args, vis=False, model=model_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    next(env_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some code to figure out inputs\n",
    "\n",
    "Looks like the first 256 tokens (i.e. 16x16) will get replaced with the outputs of the image encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_dataset[1]\n",
    "prefix = \"<image>\" + label[\"prefix\"]\n",
    "suffix = label[\"suffix\"]\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=prefix,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    #suffix = [augment_suffix(suffix)]\n",
    ")\n",
    "\n",
    "#print(label[\"suffix\"])\n",
    "#print(decoded)\n",
    "for input_name, input in inputs.items():\n",
    "    print(input_name, input.shape)\n",
    "extra = 273 - 256 \n",
    "print(extra, extra**.5)\n",
    "print(inputs[\"input_ids\"][:, 256:])\n",
    "\n",
    "\n",
    "print(processor.decode(inputs[\"input_ids\"][0, 256:]))\n",
    "tmp = processor.decode([108])\n",
    "#print(processor.tokenizer.eos_token)\n",
    "#print(processor.image_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where parameters are located\n",
    "\n",
    "from collections import defaultdict\n",
    "param_locations = defaultdict(list)\n",
    "for i in model.named_parameters():\n",
    "    #print(f\"{i[0]} -> {i[1].device}\")\n",
    "    param_locations[f\"{i[1].device}\"]= f\"{i[0]}\"\n",
    "\n",
    "for k, v in param_locations.items():\n",
    "    print(k, len(v))\n",
    "\n",
    "#print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Additional Info when using cuda\n",
    "# import torch\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "#    print(torch.cuda.get_device_properties(i).name, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cuda visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "DEVICE = torch.device('cuda')\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE, device_map=\"auto\")\n",
    "\n",
    "batch = [valid_dataset[i] for i in range(8)]\n",
    "inputs = collate_fn(batch)\n",
    "#generate_ids = model.generate(**inputs, max_length=286+30)\n",
    "trainer.model.train()\n",
    "trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n",
    "print(\"works\")\n",
    "trainer.model.train(False)\n",
    "trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n",
    "print(\"fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [valid_dataset[i] for i in range(8)]\n",
    "inputs = collate_fn(batch)\n",
    "#generate_ids = model.generate(**inputs, max_length=286+30)\n",
    "trainer.model.train()\n",
    "trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n",
    "print(\"works\")\n",
    "trainer.model.train(False)\n",
    "trainer.compute_loss(model, inputs, return_outputs=False, num_items_in_batch=416)\n",
    "print(\"fails.\")\n",
    "\n",
    "#raise ValueError\n",
    "#pass\n",
    "# orig_context_manager = trainer.compute_loss_context_manager\n",
    "# class TempTrainContext(object):\n",
    "#     def __init__(self, trainer):\n",
    "#         self.trainer = trainer\n",
    "#         self.orig_context_manager = trainer.compute_loss_context_manager\n",
    "#     def __enter__(self):\n",
    "#         self.orig_context_inst = self.orig_context_manager()\n",
    "#         self.orig_context_inst.__enter__()\n",
    "#         self.training_enter = self.trainer.model.training\n",
    "#         self.trainer.model.train()\n",
    "#     def __exit__(self, type, value, traceback):\n",
    "#         self.trainer.model.train(self.training_enter)\n",
    "#         self.orig_context_inst.__exit__(type, value, traceback)\n",
    "#     def __call__(self):\n",
    "#         return self\n",
    "# trainer.compute_loss_context_manager = TempTrainContext(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune PaliGemma2 using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title Freeze the image encoder\n",
    "\n",
    "# TORCH_DTYPE = torch.bfloat16\n",
    "# #model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE).to(DEVICE)\n",
    "# model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE, device_map=\"auto\")## max_memory={1:\"25GB\",})# 5:\"25GB\", 6:\"25GB\"})  # was auto\n",
    "\n",
    "# for param in model.vision_tower.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in model.multi_modal_projector.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @title Fine-tune the entire model with LoRA and QLoRA\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "\n",
    "# TRAIN_EXAMPLES = len(train_dataset.entries)\n",
    "# BATCH_SIZE = 18\n",
    "# BATCH_SIZE_DEV = 6\n",
    "# GRAD_ACCUM = BATCH_SIZE // BATCH_SIZE_DEV\n",
    "\n",
    "# TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "# SAVE_STEPS = TRAIN_STEPS // 8\n",
    "# SEQLEN = 32\n",
    "\n",
    "# args_lora = Seq2SeqTrainingArguments(\n",
    "#     num_train_epochs=1,\n",
    "#     remove_unused_columns=False,\n",
    "#     per_device_train_batch_size=BATCH_SIZE_DEV,\n",
    "#     gradient_accumulation_steps=GRAD_ACCUM,\n",
    "#     #gradient_checkpointing=True, use_cache=False,\n",
    "#     generation_max_length=SEQLEN,\n",
    "#     warmup_steps=2,\n",
    "#     learning_rate=.005#2e-5,\n",
    "#     weight_decay=1e-6,\n",
    "#     adam_beta2=0.999,\n",
    "#     logging_steps=10,\n",
    "#     optim=\"adamw_hf\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=1,\n",
    "#     output_dir=save_path,\n",
    "#     bf16=True,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     dataloader_pin_memory=False\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     #eval_dataset=valid_dataset,\n",
    "#     data_collator=collate_fn,\n",
    "#     args=args_lora\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "#print(save_path)\n",
    "#trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "class CustomProcessor:\n",
    "    \"\"\"\n",
    "    A wrapper around a Hugging Face Processor (e.g., PaliGemmaProcessor) that allows\n",
    "    overriding or adding token mappings according to a given dictionary of new tokens.\n",
    "\n",
    "    Args:\n",
    "        processor: A Hugging Face Processor object with a .tokenizer attribute.\n",
    "        new_tokens: Dict[str, int] mapping token strings (e.g. \"<my_token>\") to desired token IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor, new_tokens: dict):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer\n",
    "        self._override_tokens(new_tokens)\n",
    "\n",
    "    def _override_tokens(self, new_tokens: dict):\n",
    "        # Update tokenizer mappings\n",
    "        # Supports both Python and Rust tokenizers\n",
    "        enc = getattr(self.tokenizer, 'encoder', None)\n",
    "        dec = getattr(self.tokenizer, 'decoder', None)\n",
    "        vocab = getattr(self.tokenizer, 'vocab', None)\n",
    "        ids_to_tokens = getattr(self.tokenizer, 'ids_to_tokens', None)\n",
    "\n",
    "        for token, token_id in new_tokens.items():\n",
    "            # Update encoder (token -> id)\n",
    "            if enc is not None:\n",
    "                enc[token] = token_id\n",
    "            # Update vocab for Python tokenizers\n",
    "            if vocab is not None:\n",
    "                vocab[token] = token_id\n",
    "            # Update decoder (id -> token)\n",
    "            if dec is not None:\n",
    "                dec[token_id] = token\n",
    "            # Update fast tokenizer ids_to_tokens\n",
    "            if ids_to_tokens is not None:\n",
    "                ids_to_tokens[token_id] = token\n",
    "\n",
    "        # If using a fast tokenizer, ensure the tokenizer knows about special tokens\n",
    "        # so they get recognized during tokenization and decoding\n",
    "        self.tokenizer.special_tokens_map_extended = {\n",
    "            **getattr(self.tokenizer, 'special_tokens_map_extended', {}),\n",
    "            **{token: token for token in new_tokens.keys()}\n",
    "        }\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # Delegate tokenization to the underlying processor\n",
    "        return self.processor(*args, **kwargs)\n",
    "\n",
    "    def decode(self, token_ids, **kwargs):\n",
    "        # Delegate decoding, using the possibly overridden mappings\n",
    "        return self.processor.decode(token_ids, **kwargs)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        # Save both processor and tokenizer adjustments\n",
    "        self.processor.save_pretrained(save_directory)\n",
    "        self.tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "def get_last_n_tokens(processor, n=100):\n",
    "    tok = processor.tokenizer\n",
    "    try:\n",
    "        vocab = tok.get_vocab()\n",
    "    except AttributeError:\n",
    "        vocab = tok.vocab\n",
    "\n",
    "    id_to_token = {id_: t for t, id_ in vocab.items()}\n",
    "    last_ids = sorted(id_to_token)[-n:]\n",
    "    return [(i, id_to_token[i]) for i in last_ids]\n",
    "\n",
    "# Example usage:\n",
    "# from paligemma import PaliGemmaProcessor\n",
    "# base_processor = PaliGemmaProcessor.from_pretrained('...')\n",
    "# new_tokens = {'<my_token_1>': 10000, '<my_token_2>': 10001}\n",
    "# processor = CustomProcessor(base_processor, new_tokens)\n",
    "# encoded = processor('some text <my_token_1> more text')\n",
    "# decoded = processor.decode(encoded['input_ids'])\n",
    "#print(\"special tokens\", processor.tokenizer.all_special_tokens)\n",
    "#get_last_n_tokens(processor, 100+1024+128)\n",
    "\n",
    "\n",
    "my_tokens = [f\"<pos{x:03d}>\" for x in range(512)] + [f\"<dep{x:03d}>\" for x in range(128)] + [f\"<rot{x:03d}>\" for x in range(128)]\n",
    "print(len(my_tokens))\n",
    "print(my_tokens[-1])\n",
    "print(my_tokens)\n",
    "last_token = 255967 # the last token to use\n",
    "new_tokens = {token: last_token - i for i, token in enumerate(reversed(my_tokens))}\n",
    "\n",
    "# Quick check\n",
    "print(f\"Total tokens: {len(new_tokens)}\")\n",
    "print(f\"{my_tokens[0]} -> {new_tokens[my_tokens[0]]}\")\n",
    "print(f\"{my_tokens[-1]} -> {new_tokens[my_tokens[-1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<points x1=\"33.0\" y1=\"63.7\" x2=\"34.2\" y2=\"67.1\" alt=\"move\">text</points>\n",
    "#<eetraj x1=\"33.0\" y1=\"63.7\" d1=\"12.3\" ra1=\"12.3\" rb=\"12.3\" rc=\"12.3\" ... alt=\"object_name\">object_name</eetraj>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "paligemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
