{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "from cvla.utils_traj_tokens import getActionEncInstance\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "username = getpass.getuser()\n",
    "if username == \"argusm\":\n",
    "    dataset_location = Path(\"/data/lmbraid19/argusm/datasets\")\n",
    "    model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "else:\n",
    "    dataset_location = Path(\"/home/houman/cVLA_test/\")\n",
    "    model_location = Path(\"/home/houman/cVLA_test/models\")\n",
    "\n",
    "\n",
    "#dataset_location = dataset_location / \"cvla-droid-1of5c-v1\"\n",
    "\n",
    "#model_path = model_location / \"clevr-act-7-depth_text_aug\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_e512s\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"mix30obj_text_debug\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"mix30obj_depth\" / \"checkpoint-4687\"\n",
    "model_path = model_location / \"mix30obj_mask\" / \"checkpoint-4687\"\n",
    "\n",
    "\n",
    "# some processing\n",
    "info_file = model_path.parent / \"cvla_info.json\"\n",
    "try:\n",
    "    with open(info_file, \"r\") as f:\n",
    "        model_info = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    model_info = None\n",
    "\n",
    "if model_info is not None:\n",
    "    action_encoder = model_info[\"action_encoder\"]\n",
    "    return_depth = model_info[\"return_depth\"]\n",
    "else:\n",
    "    action_encoder = \"xyzrotvec-cam-1024xy\"\n",
    "    return_depth = False\n",
    "    if \"_depth\" in str(model_path):\n",
    "        return_depth = True\n",
    "\n",
    "enc_model = getActionEncInstance(action_encoder)\n",
    "dataset_name = dataset_location.name\n",
    "model_name = model_path.parent.name\n",
    "\n",
    "print()\n",
    "print(\"dataset:\".ljust(10), dataset_name, dataset_location)\n",
    "if model_path.is_dir():\n",
    "    print(\"model:\".ljust(10), model_name,\"\\t\", model_path)\n",
    "    print(\"encoder\".ljust(10), action_encoder)\n",
    "    print(\"depth:\".ljust(10), return_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from cvla.data_loader_images import ImageFolderDataset\n",
    "from cvla.utils_trajectory import DummyCamera\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "test_dataset_no_crop = ImageFolderDataset(dataset_location / \"cvla-imbit-1\", startswith=\"rgb_\")\n",
    "image_width_no_crop, image_height_no_crop = test_dataset_no_crop[0][0].size\n",
    "print(\"original image size\", image_width_no_crop, image_height_no_crop)\n",
    "\n",
    "camera_extrinsic = [[[1, 0, 0.0, 0.0], [0, 1, 0, 0], [0, 0, 1, 0]]]\n",
    "camera_intrinsic = [[[260.78692626953125, 0.0, 322.3820495605469],[ 0.0, 260.78692626953125, 180.76370239257812],[0.0, 0.0, 1.0]]]\n",
    "camera_no_crop = DummyCamera(camera_intrinsic, camera_extrinsic, width=image_width_no_crop, height=image_height_no_crop)\n",
    "\n",
    "center_crop = v2.CenterCrop(360)\n",
    "test_dataset = ImageFolderDataset(dataset_location / \"cvla-imbit-1\", startswith=\"rgb_\", transform=center_crop)\n",
    "image_width, image_height = test_dataset[0][0].size\n",
    "print(\"new image size\", image_width, image_height)\n",
    "\n",
    "# compute intrinsic matrix for cropped camera\n",
    "dx = int((image_width_no_crop - image_width) / 2)\n",
    "dy = int((image_height_no_crop - image_height) / 2)\n",
    "K = np.array(camera_intrinsic[0])  # shape (3,3)\n",
    "K_cropped = K.copy()\n",
    "K_cropped[0,2] -= dx\n",
    "K_cropped[1,2] -= dy\n",
    "camera = DummyCamera([K_cropped.tolist()], camera_extrinsic, width=image_width, height=image_height)\n",
    "\n",
    "test_dataset.labels = [dict(prefix=\"\", suffix=\"\")]*len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from cvla.utils_vis import render_example\n",
    "import torch\n",
    "\n",
    "base_to_tcp_pos = torch.tensor([[[-0.7487, -0.3278,  0.7750]]])\n",
    "base_to_tcp_orn = torch.tensor([[[ 0,  0, 0, 1]]])  # quaternion w, x, y, z \n",
    "_, _, robot_state = enc_model.encode_trajectory(base_to_tcp_pos, base_to_tcp_orn, camera)\n",
    "\n",
    "print(\"robot_state\", robot_state)\n",
    "\n",
    "def get_image(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[-1]\n",
    "    else:\n",
    "        return images\n",
    "    \n",
    "def get_depth(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(len(test_dataset))\n",
    "num_samples = min(3*1, len(test_dataset))\n",
    "html_imgs = \"\"\n",
    "for i in tqdm(range(num_samples)):\n",
    "    images, sample = test_dataset[i]\n",
    "    image = get_depth(images) if return_depth else get_image(images)\n",
    "    html_imgs += render_example(image, text=\"{i} text\"+robot_state, camera=camera, enc=enc_model)\n",
    "    \n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"loaded processor.\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_path, torch_dtype=TORCH_DTYPE, device_map=\"auto\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Data Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from math import ceil\n",
    "from cvla.utils_vis import render_example\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).to(DEVICE)\n",
    "    return inputs\n",
    "\n",
    "base_to_tcp_pos = torch.tensor([[[-0.7487, -0.3278,  0.7750]]])\n",
    "base_to_tcp_orn = torch.tensor([[[ 1,  0, 0, 0]]])  # quaternion w, x, y, z \n",
    "test_dataset.labels = [dict(prefix=\"\", suffix=\"\")]*len(test_dataset)\n",
    "_, _, robot_state = enc_model.encode_trajectory(base_to_tcp_pos, base_to_tcp_orn, camera)\n",
    "\n",
    "x = \"yellow cup\"\n",
    "y = \"plate\"\n",
    "action_text = \"put the {} inside the {}\".format(x, y)\n",
    "prefix = action_text + \" \" + robot_state\n",
    "test_dataset.labels = [dict(prefix=prefix, suffix=\"\")]*len(test_dataset)\n",
    "\n",
    "eval_batch_size = 8\n",
    "test_samples = len(test_dataset)\n",
    "pred_list = []\n",
    "html_imgs = \"\"\n",
    "for start_idx in tqdm(range(0, test_samples, eval_batch_size), total=ceil(test_samples / eval_batch_size)):\n",
    "    batch = [test_dataset[i] for i in range(start_idx, min(start_idx + eval_batch_size, test_samples))]\n",
    "    #for e in batch:\n",
    "    #    augment_sample(e)\n",
    "    inputs = collate_fn(batch)\n",
    "    prefix_length = inputs[\"input_ids\"].shape[-1]    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "        decoded = [processor.decode(x, skip_special_tokens=True) for x in generation[:, prefix_length:]]\n",
    "    pred_list.extend(decoded)\n",
    "    \n",
    "    for batch_entry, decoded_str in zip(batch, decoded):\n",
    "        if return_depth:\n",
    "            (depth, image), sample = batch_entry\n",
    "        else:\n",
    "            image, sample = batch_entry\n",
    "        \n",
    "        html_img = render_example(image, text=sample[\"prefix\"], prediction=decoded_str, camera=camera, enc=enc_model)\n",
    "        html_imgs += html_img\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dec_result = enc_model.decode_trajectory(robot_state, camera)\n",
    "base_to_tcp_pos = torch.tensor([[[-0.7487, -0.3278,  0.7750]]])\n",
    "#tcp_orn_in_camera = torch.tensor([[[ 0.5494,  0.4075, -0.5746,  0.4493]]])  # quaternion w, x, y, z \n",
    "base_to_tcp_orn = torch.tensor([[[ 0,  0, 0, 1]]])  # quaternion w, x, y, z \n",
    "_, _, robot_state_str = enc_model.encode_trajectory(base_to_tcp_pos, base_to_tcp_orn, camera)\n",
    "text = \" \"+robot_state_str\n",
    "\n",
    "html_img = render_example(np.ones_like(image)*255, text=text, label=None, prediction=None, camera=camera, enc=enc_model)\n",
    "display(HTML(html_img))\n",
    "\n",
    "print(robot_state_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
