{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from cvla.hf_model_class import cVLA_wrapped\n",
    "\n",
    "model_location = Path(\"/home/houman/cVLA_test/models\")\n",
    "#model_path = model_location / \"clevr-act-7-depth_l40\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-6-var-cam2_hf_af_lr3e5\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_rbg20\" / \"checkpoint-4687\"\n",
    "model_path = model_location / \"mix30obj_text_debug\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_rbg60\" / \"checkpoint-4687\"\n",
    "\n",
    "model_inst = cVLA_wrapped(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from PIL import Image   \n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cvla.utils_trajectory import DummyCamera\n",
    "from cvla.utils_vis import render_example\n",
    "\n",
    "def get_new_image(image_dir = Path(\"/home/houman/cVLA_test/saved_images/dataset\"), image_number=1):\n",
    "    process = subprocess.Popen(\"python /home/houman/catkin_ws/src/franka_utils/scripts/get_image.py {}\".format(image_number),shell=True)\n",
    "    time.sleep(1)\n",
    "    depth_image = np.load(image_dir / f\"depth_image_{image_number}.npy\")\n",
    "    rgb_image = Image.open( image_dir / f\"rgb_image_{image_number}.png\")\n",
    "    if process.poll() is None:\n",
    "        process.kill()\n",
    "        print(\"Process killed after 1 second.\")\n",
    "    else:\n",
    "        print(\"Process finished before timeout.\")\n",
    "\n",
    "\n",
    "    depth_image = np.load(image_dir / f\"depth_image_{image_number}.npy\")\n",
    "    rgb_image = Image.open( image_dir / f\"rgb_image_{image_number}.png\")\n",
    "    return rgb_image, depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new image\n",
    "rgb_image, depth_image = get_new_image()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Generate the prefix\n",
    "x = \"black sponge\"\n",
    "y = \"plate\"\n",
    "action_text = \"put the {} inside the {}\".format(x, y)\n",
    "\n",
    "crop = True\n",
    "center_crop = v2.CenterCrop(360)\n",
    "\n",
    "image_width, image_height = rgb_image.size\n",
    "camera_extrinsic = [[[1, 0, 0.0, 0.0], [0, 1, 0, 0], [0, 0, 1, 0]]]\n",
    "camera_intrinsic = [[[260.78692626953125, 0.0, 322.3820495605469],[ 0.0, 260.78692626953125, 180.76370239257812],[0.0, 0.0, 1.0]]]\n",
    "camera = DummyCamera(camera_intrinsic, camera_extrinsic, width=image_width, height=image_height)\n",
    "\n",
    "base_to_tcp_pos = torch.tensor([[[-0.7487, -0.3278,  0.7750]]])\n",
    "base_to_tcp_orn = torch.tensor([[[ 1,  0, 0, 0]]])  # quaternion w, x, y, z \n",
    "\n",
    "_, _, robot_state = model_inst.enc_model.encode_trajectory(base_to_tcp_pos, base_to_tcp_orn, camera)\n",
    "\n",
    "if model_inst.return_depth:\n",
    "    images = [rgb_image, depth_image]\n",
    "    print(images[0].shape, images[0].dtype)  # (720, 1280, 3) uint8 depth image encoded\n",
    "    print(images[1])  # <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at 0x715AEF7667B0>\n",
    "else:\n",
    "    images = rgb_image\n",
    "    if crop:\n",
    "        images = center_crop(images)\n",
    "\n",
    "\n",
    "pred_text = model_inst.predict(images, action_text, robot_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decc_model = model_inst.enc_model.decode_caption\n",
    "sample = dict(suffix=pred_text[0], prefix=action_text + \" \" + robot_state)\n",
    "\n",
    "image_crop_width, image_crop_height = images.size\n",
    "camera_after_crop = DummyCamera(camera_intrinsic, camera_extrinsic, width=image_crop_width, height=image_crop_height)\n",
    "\n",
    "print(sample[\"prefix\"])\n",
    "print(\"pred_text\", pred_text)\n",
    "html_imgs = \"\"\n",
    "html_imgs += render_example(images, label=sample[\"suffix\"], text=sample[\"prefix\"], camera=camera_after_crop, enc=model_inst.enc_model)    \n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_closest_valid_pixel(depth_image, target_row, target_col):\n",
    "    \"\"\"\n",
    "    Find the closest valid pixel value to the target position in a depth image.\n",
    "    \n",
    "    Args:\n",
    "        depth_image: 2D numpy array representing the depth image\n",
    "        target_row: Row index of the position to find closest valid pixel for\n",
    "        target_col: Column index of the position to find closest valid pixel for\n",
    "    \n",
    "    Returns:\n",
    "        float: Value of the closest valid pixel, or None if no valid pixels exist\n",
    "    \"\"\"\n",
    "    rows, cols = depth_image.shape\n",
    "    print (rows, cols)\n",
    "    if target_row >= rows or target_col >= cols:\n",
    "        raise ValueError(\"Target coordinates exloc0476ceed image dimensions\")\n",
    "    closest_pixel_coords = None\n",
    "    # Get coordinates of all valid pixels\n",
    "    valid_coords = [(i,j) for i in range(rows) \n",
    "                    for j in range(cols) \n",
    "                    if not np.isnan(depth_image[i,j])]\n",
    "    \n",
    "    # If target posienc_modeltion already has a valid value, return it\n",
    "    if not np.isnan(depth_image[target_row, target_col]):\n",
    "        print(\"The coordinate already has a Non-Nan value!\")\n",
    "        return depth_image[target_row, target_col], [target_row, target_col]\n",
    "    \n",
    "    # If no valid pixels exist, return None\n",
    "    if not valid_coords:\n",
    "        return None, None\n",
    "    \n",
    "    # Find closest valid pixel using Manhattan distance\n",
    "    min_dist = float('inf')\n",
    "    closest_value = None\n",
    "    \n",
    "    for row, col in valid_coords:\n",
    "        dist = abs(target_row - row) + abs(target_col - col)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_value = depth_image[row, col]\n",
    "            closest_pixel_coords = [row,col]\n",
    "            \n",
    "    return closest_value, closest_pixel_coords\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Test finding closest value for a NaN position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pred_text)\n",
    "traj_c = model_inst.enc_model.decode_caption(pred_text[0], camera=camera)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "traj_c_fixed = deepcopy(traj_c)\n",
    "#traj_c_fixed[0]\n",
    "\n",
    "#color_value = np.asanyarray(rgb_image)[point[1], point[0]]\n",
    "points = []\n",
    "for point_idx in (0,1):\n",
    "    point = np.array(traj_c[0][point_idx,:2].round().numpy(), dtype=int)\n",
    "    row, col = point[1], point[0]  # Position containing NaN\n",
    "    closest_value, closest_pixel_coords = find_closest_valid_pixel(depth_image, row, col)\n",
    "    print(f\"Closest valid value to position ({row}, {col}): {closest_value} at ({closest_pixel_coords[0]}, {closest_pixel_coords[1]})\")\n",
    "    depth_value = closest_value #depth_image[point[1], point[0]]\n",
    "\n",
    "    print(\"point\", point, depth_value)\n",
    "    points.append(point)\n",
    "\n",
    "    # if point_idx == 0:  # TODO(houman): find some good values, higher=further\n",
    "    #     depth_value += 0.02\n",
    "    # elif point_idx == 1:\n",
    "    #     depth_value -= 0.03\n",
    "    # else:\n",
    "    #     raise ValueError\n",
    "\n",
    "    print(\"depth_value\", depth_value)\n",
    "    traj_c_fixed[0][point_idx,2] = float(depth_value)\n",
    "\n",
    "print(\"old\", traj_c[0])\n",
    "print(\"fixed\", traj_c_fixed[0])\n",
    "print()\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from mani_skill.utils.structs import Pose\n",
    "from mani_skill.examples.utils_trajectory import unproject_points\n",
    "\n",
    "curve_25d, quat_c = traj_c_fixed\n",
    "# from camera to world coordinates\n",
    "extrinsic_orn = R.from_matrix(camera.get_extrinsic_matrix()[:, :3, :3])\n",
    "extrinsic = Pose.create_from_pq(p=camera.get_extrinsic_matrix()[:, :3, 3],\n",
    "                                q=extrinsic_orn.as_quat(scalar_first=True))\n",
    "quat_w = extrinsic.inv() * Pose.create_from_pq(q=quat_c)\n",
    "curve_w = unproject_points(camera, curve_25d) \n",
    "\n",
    "curve_w, quat_w.get_q().unsqueeze(0)  # shape (P, 3 = u, v, d)    \n",
    "\n",
    "print(\"done.\")\n",
    "print(curve_w)\n",
    "print(quat_w)\n",
    "    \n",
    "print (np.shape(quat_w))\n",
    "print (np.shape(quat_w.raw_pose.numpy()))\n",
    "print (points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#point = (360, 170) #width, height yellow point works\n",
    "#point = (355, 170) #width, height yellow point fails\n",
    "print (points[0][0])\n",
    "point_1 = (points[0][0], points[0][1]) #width, height\n",
    "point_2 = (points[1][0], points[1][1]) #width, height\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(rgb_image)\n",
    "axs[0].plot(point_1[0], point_1[1],'.-', color='lime')\n",
    "axs[0].plot(point_2[0], point_2[1],'.-', color='red')\n",
    "\n",
    "axs[1].imshow(depth_image)\n",
    "axs[1].plot(point_1[0], point_1[1],'.-', color='lime')\n",
    "axs[1].plot(point_2[0], point_2[1],'.-', color='red')\n",
    "\n",
    "# color_value = np.asanyarray(rgb_image)[point[1], point[0]]\n",
    "# depth_value = depth_image[point_2[1], point[0]]\n",
    "# print (depth_image.shape)\n",
    "\n",
    "# if depth_value is np.nan:\n",
    "#     sample_range = # get dxd pixels centered at point\n",
    "#     #get all depth_values\n",
    "#     # get max of depth value\n",
    "print(\"XXX\", depth_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. Maybe sample depth range by default (?)\n",
    "2. Remeber to do grasp point depth + 2.5 cm(?), placement depth - 5/10cm(?)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Option 1:\n",
    "(x,y,z) positions -> encode -> string -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cVLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
