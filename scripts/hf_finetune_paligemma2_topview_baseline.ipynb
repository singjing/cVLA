{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LqvmtZPzyY1"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Fine-tune PaliGemma2 on Object Detection Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md)\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)\n",
    "\n",
    "PaliGemma 2 is built by combining the SigLIP-So400m vision encoder with the more recent and capable language models from the Gemma 2 family.\n",
    "\n",
    "![PaliGemma2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-1.png)\n",
    "\n",
    "The authors use a 3-stage training approach similar to the original PaliGemma. In stage 1, they combine the pretrained vision and language model components and train them jointly on a multimodal task mixture. In stage 2, they train the models at higher resolutions of 448px^2 and 896px^2. In stage 3, they fine-tune the models on the target transfer tasks.\n",
    "\n",
    "PaliGemma 2 models outperform the original PaliGemma at the same resolution and model size. Increasing the model size and resolution generally improves performance across a wide range of tasks, but the benefits differ depending on the task. Some tasks benefit more from increased resolution, while others benefit more from a larger language model.\n",
    "\n",
    "![PaliGemma2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-2.png)\n",
    "\n",
    "Notebook requires A100 with 40GB of VRAM to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBp3Czz3GBmc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADTkh-2y_9Yv"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To fine-tune PaliGemma2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
    "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
    "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
    "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wyojKiG_hX9"
   },
   "source": [
    "### Select the runtime\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_8BLW6R_x-z",
    "outputId": "678f6080-89ea-41f7-eecb-a592d1a03d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 21 07:35:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:88:00.0 Off |                  N/A |\n",
      "|  0%   25C    P8             22W /  250W |       1MiB /  11264MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   #,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMlw3ru1YvLg"
   },
   "source": [
    "### Download dataset from Roboflow Universe\n",
    "\n",
    "To fine-tune PaliGemma2, prepare your dataset in JSONL format. You can use Roboflow to easily convert any dataset into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtvz4QZ9YuG8",
    "outputId": "8edb179f-f6a1-421d-f50a-2ee61724415a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "sending incremental file list\n",
      "/tmp/indoorCVPR: directory\n",
      "/tmp/training2: directory\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q peft bitsandbytes transformers==4.47.0 tf-keras\n",
    "!rsync -a --progress /data/lmbraid19/argusm/datasets/indoorCVPR_09.tar /tmp/ && mkdir -p /tmp/indoorCVPR && tar -xf /tmp/indoorCVPR_09.tar -C /tmp/indoorCVPR\n",
    "!rsync -a --progress /work/dlclarge2/zhangj-zhangj-CFM/data/training2 /tmp/\n",
    "!file /tmp/indoorCVPR\n",
    "!file /tmp/training2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ElnTKvBVa"
   },
   "source": [
    "**NOTE:** Let's read the first few lines of the annotation file and examine the dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLhSenP5AtQe",
    "outputId": "f6a6b7f0-6360-4eaf-8abf-bd40462af58a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_location: /tmp/training2 samples: 88244\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_loader_jsonl import JSONLDataset\n",
    "from cvla.data_augmentations import augment_image_rgb, RandomizeBackgrounds\n",
    "from cvla.data_augmentations import complexify_text, DepthAugmentation\n",
    "from cvla.data_loader_images import ImageFolderDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "dataset_location = Path(\"/tmp/training2\")\n",
    "\n",
    "bg_image_dataset = ImageFolderDataset(\"/tmp/indoorCVPR/Images\", transform=transforms.RandomResizedCrop((448,448)))\n",
    "randomize_background = RandomizeBackgrounds(p=0.2, background_images=bg_image_dataset)\n",
    "augment_depth = DepthAugmentation(depth_range=(25, 100), max_delta_depth=35)\n",
    "train_dataset = H5Dataset(dataset_location, augment_rgb=augment_image_rgb, augment_text=complexify_text,\n",
    "                          augment_depth=augment_depth, return_depth=False,action_encoder=\"xyzrotvec-cam-512xy\")\n",
    "#augment_rgbds=randomize_background,\n",
    "\n",
    "print(\"dataset_location:\", dataset_location,\"samples:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McU9159EvkeA"
   },
   "source": [
    "### Set up and test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "19pQQjIixL-T",
    "outputId": "c59a8054-4f92-4cb0-b1cb-6a7b89e6e4b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getActionEncInstance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcvla\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils_vis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m render_example\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m enc = \u001b[43mgetActionEncInstance\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mxyzrotvec-cam-512xy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m num_samples = \u001b[32m3\u001b[39m*\u001b[32m2\u001b[39m\n\u001b[32m      6\u001b[39m html_imgs = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'getActionEncInstance' is not defined"
     ]
    }
   ],
   "source": [
    "from cvla.utils_vis import render_example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "enc = getActionEncInstance(\"xyzrotvec-cam-512xy\")\n",
    "num_samples = 3*2\n",
    "html_imgs = \"\"\n",
    "for i in range(num_samples):\n",
    "    image, sample = train_dataset[i]\n",
    "    prefix = sample[\"prefix\"]\n",
    "    #html_imgs += render_example(image[0], label=sample[\"suffix\"], text=prefix, camera=sample[\"camera\"])\n",
    "    #html_imgs += render_example(image[1], label=sample[\"suffix\"], text=prefix, camera=sample[\"camera\"])\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], enc=enc, text=prefix, camera=sample[\"camera\"])\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvYNxYbBtE3"
   },
   "source": [
    "### Load PaliGemma2 model\n",
    "\n",
    "**NOTE:** PaliGemma2 offers 9 pre-trained models with sizes of `3B`, `10B`, and `28B` parameters, and resolutions of `224`, `448`, and `896` pixels. In this tutorial, I'll be using the [`google/paligemma2-3b-pt-448`](https://huggingface.co/google/paligemma2-3b-pt-448) checkpoint. Resolution has a key impact on the mAP of the trained model, and it seems that `448` offers the most optimal balance between performance and compute resources required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "a5cd856a1fbf413a9aa4edff7cf3e636",
      "23be9405ad7f4cd8817db36f2f3834dd",
      "2e2e11bda8024f97868d45598bff6dc8",
      "11a966b75e124f369844afda5db25ab6",
      "37650cf71172405392ab843fd82360c3",
      "4fc0f4ab93ff4fddb482a8d6820b1253",
      "cda1f28093f04b1ca2e3689d1e314e10",
      "972c99a291d24b9f96ea09df63d5cf4b",
      "83aa606372b848659e13d683fa2fe09e",
      "3c26b58a776346f2920f810d99b21545",
      "fc9c7ae18f7341598a741ee8811896ff",
      "632c3fbb8739429b875687049eb1cf61",
      "7aeec69c06b049738c1fae480103cb1d",
      "c4e5a6e67ec3443fb73bfa86a27df704",
      "9355cb34cea948909504c1144169b693",
      "7a294c9159804eb388a2d7fe67f2920e",
      "7877bed5b8144f78a5a3f8b33741a82b",
      "dcb6f37a97664ed8ad2a699956bd772b",
      "ee2fa3db801344ba8564f700ca5ee2c5",
      "d5b373e9c1fc4d078e5512764235c6ba"
     ]
    },
    "id": "VfCgxIp3EjmC",
    "outputId": "db59a5d0-440f-4ca3-c5a3-194b9e22f7bd"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2f877f70e9b241f08717bba2b7616198",
      "9393bb98f83e4bf1bd8becdfdfb41f12",
      "41a50a96ca224e0aa12ef142d317e762",
      "cfcdcf908ff04fa19159c98b08c851ce",
      "9165a3ed1f2d4671b089dd00cae0fdbb",
      "6873590d9ac9436facef6cb182e1c077",
      "7ef24d8b5077400b9e005b373ab8d4f1",
      "df5fb754518547b480d95ce715bcdd8b",
      "c3eddb53526e4694bbf5ee3636e1b806",
      "eaf748f0d17148eba7e6351d107c1a50",
      "625fb0f94f5546e0a643192c2bd6c40f",
      "45db0bb8841f48e38c8eb8d28c1e72b8",
      "254a4ab182234215a7aeb6873f6c62af",
      "b7c2a5216879471c982406086571af7e",
      "9db3a6d6cd9548878d0998150f897ff1",
      "3226844ff08a46dcbc982432456d496b",
      "94f8f306b995471291fc610c4f170b26",
      "b91043be62ef4965852d9dcf5faf8edc",
      "141bc95f2dcc485cb9d23f0f01d28f24",
      "a770c3f09c204bf7b879a0d72256977d",
      "545d3d664c8e49cab3270012be39a604",
      "eeb274cb895f416bb85211d8ed07c005",
      "a3be6c18dd6440e48e80a9dc70bf9c9a",
      "f6619c93a7274805bb3005b0898d5f5f",
      "ef7ffed4ac344f7aaf1f39417284f4e3",
      "eab62ec555b146f4802630f6dd38afc6",
      "3dd89974f8b74268bf295119037ab0b8",
      "b18537a50e8f4e4d9829735ea2a98596",
      "d7f77f6a75f9441b8b1231b2eec3123a",
      "9f36c016c2a24d398e84c62964c47277",
      "dce11ec58ce84d9ea0ae3ac26519e796",
      "1a33f188e7ce499ebcb96e7e1617efe4",
      "be391a3e32d04aea837517041123cece",
      "4dd7115fec054f4790524d469bb263f7",
      "b6a7688a29f74e18b9c10ca185585457",
      "ed85f01229c146a48ae86370c36fb588",
      "2e5cf178fc03434faad8539094ddce4a",
      "ae8a2d84dd28458ba68bc94553337064",
      "20f03fcb438749f5a95cd3a72246fafd",
      "6f9622e6bdb1478e99ddddd1e2257f01",
      "f6e321f3c19f41688f3b2fd84243e851",
      "52d311c2b3af4979b433a392e77b5b21",
      "bddb4ea70dab445badbd5a4b2ef66ed4",
      "e9eb6315ff8a42f4911a3cee35e8320b"
     ]
    },
    "id": "ntXj4A3SyEAa",
    "outputId": "898b4343-0089-4ee6-8a54-d1f76d39b59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda visible devices: 0\n",
      "cuda\n",
      "Using device: cuda\n",
      "Good devices [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f36fc2667045208a1f1c5b08f920df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "print(\"cuda visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "devices_good = sorted((int(x) for x in os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")))\n",
    "DEVICE = torch.device('cuda')\n",
    "print(DEVICE)\n",
    "print('Using device:', DEVICE)\n",
    "print(\"Good devices\", devices_good)\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "# use checkpoint\n",
    "#LOCAL_CHECKPOINT = \"/data/lmbraid19/argusm/models/_text_lr3e-05xyzrotvec-cam-512xy256d_2025-04-23_12-03-48/checkpoint-4687\"\n",
    "\n",
    "#fine-tune directly on paligemma2\n",
    "MODEL_NAME = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "processor = PaliGemmaProcessor.from_pretrained(\"google/paligemma2-3b-pt-224\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    #LOCAL_CHECKPOINT,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "#.to(\"cuda\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([3, 292])\n",
      "token_type_ids torch.Size([3, 292])\n",
      "attention_mask torch.Size([3, 292])\n",
      "pixel_values torch.Size([3, 3, 224, 224])\n",
      "labels torch.Size([3, 292])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)#.to(DEVICE)\n",
    "    print(\"prefixes\", prefixes)\n",
    "    return inputs\n",
    "'''\n",
    "batch = [train_dataset[i] for i in range(3)]\n",
    "inputs = collate_fn(batch)\n",
    "for x in inputs:\n",
    "    print(x, inputs[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   1065,    573,   2674,  94764,\n",
       "          14768,    575,    573,  66635,  45839,   2237, 235248, 256516, 257023,\n",
       "         256044, 257063, 257092, 257094,    108, 256656, 256509, 256058, 257062,\n",
       "         257088, 257088, 256496, 256721, 256065, 257062, 257088, 257088,      1],\n",
       "        [257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   3531,    573,  42536, 103726,\n",
       "          18323,  19310,    575,    573,   3118,    798, 235248, 256617, 256711,\n",
       "         256053, 257069, 257103, 257113,    108, 256665, 256557, 256058, 257069,\n",
       "         257104, 257114, 256472, 256792, 256065, 257069, 257104, 257114,      1],\n",
       "        [257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   1065,    573,  31612,  15811,\n",
       "         158348,   1280,    573,  47074,  11769,  38799, 235248, 256543, 256723,\n",
       "         256049, 257063, 257100, 257102,    108, 256729, 256799, 256059, 257072,\n",
       "         257116, 257123, 256560, 257023, 256052, 257072, 257116, 257123,      1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef collate_fn(batch):\\n    images, labels = zip(*batch)\\n    prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\\n    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\\n    images_flat = [img for img_list_x in images for img in img_list_x]\\n    inputs = processor(\\n        text=prefixes,\\n        images=images_flat,\\n        return_tensors=\"pt\",\\n        suffix=suffixes,\\n        padding=\"longest\"\\n    ).to(TORCH_DTYPE)\\n    return inputs\\n\\nbatch = [train_dataset[i] for i in range(3)]\\ninputs = collate_fn(batch)\\nfor x in inputs:\\n    print(x, inputs[x].shape)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "    images_flat = [img for img_list_x in images for img in img_list_x]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images_flat,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)\n",
    "    return inputs\n",
    "\n",
    "batch = [train_dataset[i] for i in range(3)]\n",
    "inputs = collate_fn(batch)\n",
    "for x in inputs:\n",
    "    print(x, inputs[x].shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   1065,    573,   2674,  94764,\n",
       "          14768,   5580,    573,  66635,  45839,   2237, 235248, 256516, 257023,\n",
       "         256044, 257063, 257092, 257094,    108, 256656, 256509, 256058, 257062,\n",
       "         257088, 257088, 256496, 256721, 256065, 257062, 257088, 257088,      1],\n",
       "        [257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   3531,    573,  42536, 103726,\n",
       "          18323,  19310,   5580,    573,   3118,    798, 235248, 256617, 256711,\n",
       "         256053, 257069, 257103, 257113,    108, 256665, 256557, 256058, 257069,\n",
       "         257104, 257114, 256472, 256792, 256065, 257069, 257104, 257114,      1],\n",
       "        [257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152,\n",
       "         257152, 257152, 257152, 257152,      2,   1065,    573,  31612,  15811,\n",
       "         158348,    575,    573,  47074,  11769,  38799, 235248, 256543, 256723,\n",
       "         256049, 257063, 257100, 257102,    108, 256729, 256799, 256059, 257072,\n",
       "         257116, 257123, 256560, 257023, 256052, 257072, 257116, 257123,      1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, label_tokens = eval_pred  # Extract predictions and labels\n",
    "    if isinstance(predictions, tuple):  # Some models return tuples\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Convert to token indices if necessary (e.g., for text generation models)\n",
    "    pred_tokens = np.argmax(predictions, axis=-1)  # Assuming logits, take argmax\n",
    "\n",
    "    pred_texts = processor.tokenizer.batch_decode(pred_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "    label_text = processor.tokenizer.batch_decode(label_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "\n",
    "    print(pred_tokens[:,-SEQLEN-1:])\n",
    "    print(label_tokens[:,-SEQLEN-1:])\n",
    "    print(label_text)\n",
    "    print(pred_texts)\n",
    "    print()\n",
    "    return {\"accuracy\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with JAX settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path /work/dlclarge2/zhangj-zhangj-CFM/models/training2_topview_70000_baseline\n",
      "TRAIN_STEPS 5515\n",
      "GRAD_ACCUM 8\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        if \"self_attn\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "TRAIN_EXAMPLES = len(train_dataset)\n",
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_DEV = 2 # on l40 was 8\n",
    "GRAD_ACCUM = int(round(BATCH_SIZE / BATCH_SIZE_DEV))\n",
    "TRAIN_STEPS = (TRAIN_EXAMPLES // BATCH_SIZE)\n",
    "SEQLEN = 12\n",
    "SAVE_STEPS = int(TRAIN_STEPS / 15)\n",
    "SAVE_LIMIT = 5\n",
    "\n",
    "run_name = \"_topview_70000_baseline\"\n",
    "new_model_location = Path(\"/work/dlclarge2/zhangj-zhangj-CFM/models\")\n",
    "save_path = new_model_location / (str(Path(dataset_location).stem) + run_name)\n",
    "print(\"save_path\", save_path)\n",
    "print(\"TRAIN_STEPS\",TRAIN_STEPS)\n",
    "print(\"GRAD_ACCUM\", GRAD_ACCUM)\n",
    "\n",
    "args_jax = Seq2SeqTrainingArguments(\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE_DEV,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=3e-5,  # 1e-5, 2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=.05,\n",
    "    generation_max_length=SEQLEN,\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    output_dir=save_path,\n",
    "    bf16=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    #dataloader_prefetch_factor=2,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=4,\n",
    "    #per_device_eval_batch_size=BATCH_SIZE_DEV,\n",
    "    #eval_accumulation_steps=GRAD_ACCUM\n",
    ")\n",
    "#gradient_checkpointing=True,\n",
    "#weight_decay=3e-7,\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=train_dataset_small,\n",
    "    data_collator=collate_fn,\n",
    "    args=args_jax,\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only when recover from last time training\n",
    "#last_checkpoint = \"/work/dlclarge2/zhangj-zhangj-CFM/models/training2_topview_70000_based/checkpoint-183\"\n",
    "#trainer.train(resume_from_checkpoint=last_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in inputs.items():\n",
    "    if torch.is_tensor(value):\n",
    "        inputs[key] = value.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 495.12 MiB is free. Including non-PyTorch memory, this process has 10.08 GiB memory in use. Of the allocated memory 9.37 GiB is allocated by PyTorch, and 535.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2524\u001b[39m context = (\n\u001b[32m   2525\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2526\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2527\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2529\u001b[39m )\n\u001b[32m   2530\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2531\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2534\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2535\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2536\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2537\u001b[39m ):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2539\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:3678\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3676\u001b[39m         loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs)\n\u001b[32m   3677\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3678\u001b[39m         loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3680\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3681\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3682\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3683\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3684\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:3734\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3732\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3733\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3734\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3735\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3736\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/models/paligemma/modeling_paligemma.py:554\u001b[39m, in \u001b[36mPaliGemmaForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# we use the input attention mask to shift the logits and labels, because it is 2D.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\u001b[39;00m\n\u001b[32m    553\u001b[39m     shift_attention_mask = attention_mask[:, -shift_logits.shape[\u001b[32m1\u001b[39m] :].to(logits.device)\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     shift_logits = \u001b[43mshift_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshift_attention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.contiguous()\n\u001b[32m    555\u001b[39m     shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != \u001b[32m0\u001b[39m].contiguous()\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 564.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 495.12 MiB is free. Including non-PyTorch memory, this process has 10.08 GiB memory in use. Of the allocated memory 9.37 GiB is allocated by PyTorch, and 535.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "for k, v in inputs.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"  {k}: {v.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)\n",
    "print({k: v.device for k, v in inputs.items() if torch.is_tensor(v)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
