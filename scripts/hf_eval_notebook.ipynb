{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from cvla.utils_traj_tokens import getActionEncInstance\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "dataset_location = \"/tmp/cvla-clevr-8\"\n",
    "#dataset_location = \"/data/lmbraid19/argusm/datasets/cvla-droid-block-simple-v3\"\n",
    "#dataset_location = \"/data/lmbraid19/argusm/datasets/cvla-droid-block-v3\"\n",
    "dataset_location = \"/data/lmbraid19/argusm/datasets/cvla-droid-1of5c-v1\"\n",
    "dataset_location = Path(dataset_location)\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models/\")\n",
    "#model_path = model_location / \"clevr-act-7-depth_text_aug\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_e512s\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"mix30obj_text_debug\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"mix30obj_depth\" / \"checkpoint-4687\"\n",
    "model_path = model_location / \"mix30obj_mask\" / \"checkpoint-4687\"\n",
    "\n",
    "#model_path = model_location / \"clevr-act-7-depth_e512s_depth\" / \"checkpoint-4687\"\n",
    "#model_path = model_location / \"clevr-act-7-depth_depthaug\" / \"checkpoint-4687\"\n",
    "\n",
    "# some processing\n",
    "info_file = model_path.parent / \"cvla_info.json\"\n",
    "try:\n",
    "    with open(info_file, \"r\") as f:\n",
    "        model_info = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    model_info = None\n",
    "\n",
    "if model_info is not None:\n",
    "    action_encoder = model_info[\"action_encoder\"]\n",
    "    return_depth = model_info[\"return_depth\"]\n",
    "else:\n",
    "    action_encoder = \"xyzrotvec-cam-1024xy\"\n",
    "    return_depth = False\n",
    "    if \"_depth\" in str(model_path):\n",
    "        return_depth = True\n",
    "\n",
    "enc_model = getActionEncInstance(action_encoder)\n",
    "dataset_name = dataset_location.name\n",
    "model_name = model_path.parent.name\n",
    "\n",
    "print()\n",
    "print(\"dataset:\".ljust(10), dataset_name, dataset_location)\n",
    "if model_path.is_dir():\n",
    "    print(\"model:\".ljust(10), model_name,\"\\t\", model_path)\n",
    "    print(\"encoder\".ljust(10), action_encoder)\n",
    "    print(\"depth:\".ljust(10), return_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvla.data_loader_jsonl import JSONLDataset\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_augmentations import CleanText, CropMiddle\n",
    "\n",
    "\n",
    "if \"-droid-\" in str(dataset_location):\n",
    "    #crop_augmentation = CropMiddle(crop_size=600, object_size=100, valid=True)\n",
    "    crop_augmentation = None\n",
    "    clean_text = CleanText(truncate_len=75)\n",
    "    test_dataset = JSONLDataset(\n",
    "        jsonl_file_path=dataset_location,\n",
    "        action_encoder=\"xyzrotvec-cam-1024xy\",\n",
    "        augment_text=clean_text,\n",
    "        return_depth=return_depth,\n",
    "        limit_samples=200,\n",
    "        augment_crop=crop_augmentation,\n",
    "        split=\"valid\"\n",
    "    )\n",
    "else:\n",
    "    test_dataset = H5Dataset(dataset_location, return_depth=return_depth, action_encoder=action_encoder)\n",
    "    \n",
    "print(\"dataset len\", len(test_dataset))\n",
    "enc_model = enc_model\n",
    "enc_data = test_dataset.action_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from cvla.utils_vis import render_example\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_image(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[-1]\n",
    "    else:\n",
    "        return images\n",
    "    \n",
    "def get_depth(images):\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        return images[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "print(len(test_dataset))\n",
    "num_samples = min(3*2, len(test_dataset))\n",
    "html_imgs = \"\"\n",
    "for i in tqdm(range(num_samples)):\n",
    "    images, sample = test_dataset[i]\n",
    "    image = get_depth(images) if return_depth else get_image(images)\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], text=sample[\"prefix\"], camera=sample[\"camera\"], enc=enc_data, enc_pred=enc_model)\n",
    "\n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"loaded processor.\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_path, torch_dtype=TORCH_DTYPE, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "    return inputs\n",
    "\n",
    "if return_depth:\n",
    "    def collate_fn(batch):\n",
    "        images, labels = zip(*batch)\n",
    "        prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\n",
    "        images_flat = [img for img_list_x in images for img in img_list_x]\n",
    "        inputs = processor(\n",
    "            text=prefixes,\n",
    "            images=images_flat,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\"\n",
    "        ).to(TORCH_DTYPE).to(DEVICE)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from tqdm.notebook import tqdm\n",
    "from cvla.utils_vis import render_example\n",
    "\n",
    "eval_batch_size = 8\n",
    "\n",
    "#test_samples = eval_batch_size*3\n",
    "test_samples = min(len(test_dataset), 160)\n",
    "predictions = {}\n",
    "for start_idx in tqdm(range(0, test_samples, eval_batch_size), total=ceil(test_samples / eval_batch_size)):\n",
    "    batch_i = range(start_idx, min(start_idx + eval_batch_size, test_samples))\n",
    "    batch = [test_dataset[i] for i in batch_i]\n",
    "    inputs = collate_fn(batch)\n",
    "    prefix_length = inputs[\"input_ids\"].shape[-1]    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=12, do_sample=False, use_cache=False)\n",
    "        decoded = [processor.decode(x, skip_special_tokens=True) for x in generation[:, prefix_length:]]\n",
    "        \n",
    "    for k,v in zip(batch_i, decoded):\n",
    "        predictions[k] = v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = 160\n",
    "\n",
    "html_imgs = \"\"\n",
    "for _, (dataset_index, decoded_str) in zip(range(plot_images),predictions.items()):\n",
    "    batch_entry = test_dataset[dataset_index]\n",
    "    if return_depth:\n",
    "        (depth, image), sample = batch_entry\n",
    "    else:\n",
    "        image, sample = batch_entry\n",
    "    html_img = render_example(image, text=sample[\"prefix\"], label=sample[\"suffix\"], prediction=decoded_str, camera=sample[\"camera\"],\n",
    "                              enc=enc_data, enc_pred=enc_model)\n",
    "    html_imgs += html_img\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(html_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from cvla.utils_eval import Evaluator, check_if_valid\n",
    "\n",
    "camera = test_dataset[0][1][\"camera\"]\n",
    "evaluator = Evaluator(enc_model, eval_dummy_camera=camera, encoder_labels=test_dataset.action_encoder)\n",
    "\n",
    "\n",
    "for i, pred in predictions.items():\n",
    "    entry = test_dataset[i][1] \n",
    "    if i == 0:\n",
    "        print(\"image size:\", entry[\"camera\"].width, entry[\"camera\"].height)\n",
    "    suffix = entry[\"suffix\"]\n",
    "    camera = entry[\"camera\"]\n",
    "\n",
    "    if not check_if_valid(pred, suffix):   # either not enough tokens or wrong order\n",
    "            print(\"skipping\", i, pred, len(pred))\n",
    "            continue\n",
    "    \n",
    "    evaluator.evaluate(pred, suffix, camera)\n",
    "    \n",
    "    \n",
    "stats_report = evaluator.report_stats()\n",
    "\n",
    "valid_diffs = evaluator.validq_diffs\n",
    "\n",
    "keypoint = [\"object\",\"container\"]\n",
    "action_labels = [\"x\",\"y\",\"d\",\"orn\"]*2\n",
    "units = dict(cam=[\"px\",\"px\",\"cm\",\"deg\"]*2, cart=[\"cm\",\"cm\",\"cm\",\"deg\"]*2)\n",
    "\n",
    "plot_hist = True\n",
    "if plot_hist:\n",
    "    for mode, valid_diff in valid_diffs.items():\n",
    "        valid_diff = valid_diffs[mode]\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 12*1/4))  # 3 rows x 4 columns of histograms\n",
    "        axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "        for i in range(2):\n",
    "            for j in range(4):  # x, y, d\n",
    "                axes[j].hist(valid_diff[:, i,j], bins=20, alpha=0.7,  edgecolor='black', label=keypoint[i])\n",
    "                axes[j].set_title(f'Hist. {action_labels[j]} err.')\n",
    "                axes[j].set_xlabel(f\"{action_labels[j]} err. [{units[mode][j]}]\")\n",
    "                axes[j].set_ylabel('Frequency')\n",
    "                axes[j].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "depth_str = \"depth\" if return_depth else \"rgb   \"\n",
    "print(f\"model {model_name} ({depth_str})\\ndataset {dataset_name}\")\n",
    "print(f\"valid_samples\", len(valid_diffs[\"cam\"]), \"samples\", len(predictions), \"valid_rate\", f\"{len(valid_diffs[\"cam\"])/len(predictions):0.2f}\")\n",
    "for mode, valid_diff in valid_diffs.items():\n",
    "    for i, action_label in enumerate(action_labels[:4]):\n",
    "        print(f\"{action_label.ljust(2)} l2: {np.linalg.norm(valid_diff[:,:,i]):0.2f} {units[mode][i]} l1: {np.mean(np.abs(valid_diff[:,:,i])):0.2f} {units[mode][i]}\")\n",
    "    l1 = np.mean(np.abs(valid_diff))\n",
    "    l1_2d = np.mean(np.abs(valid_diff[:,:,:2]))\n",
    "    l1_depth = np.mean(np.abs(valid_diff[:,:,2]))\n",
    "    l1_depth_obj = np.mean(np.abs(valid_diff[:,0,2]))\n",
    "    out_str = f\"{model_name} ({depth_str}) L1: {l1:0.3f} L1_2d {l1_2d:0.3f} \"\n",
    "    out_str += f\"{units[mode][0]} L1_depth: {l1_depth:0.3f} {units[mode][2]} L1_depth_obj: {l1_depth_obj:0.3f} {units[mode][2]} {mode} \"\n",
    "    out_str += f\"{dataset_name}-len={len(test_dataset)}\"\n",
    "    print(out_str)\n",
    "    print()\n",
    "\n",
    "stats_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MOVE_REGEX = re.compile(r\"^move\\s+([\\w',.-]+(?:\\s+[\\w',.-]+)*)\\s+onto\\s+([\\w',.-]+(?:\\s+[\\w',.-]+)*)$\")\n",
    "\n",
    "text = \"move mr. potohead onto blue block\"\n",
    "match = MOVE_REGEX.match(text.strip())\n",
    "print(match.groups())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mix30obj_text (rgb   ) L1: 75.920 L1_2d 60.521 px L1_depth: 14.226 cm L1_depth_obj: 13.250 cm cam cvla-droid-1of5c-v1-len=200\n",
    "clevr-act-7-depth_e512s (rgb   ) L1: 77.966 L1_2d 67.927 px L1_depth: 12.432 cm L1_depth_obj: 11.803 cm cam cvla-droid-1of5c-v1-len=200\n",
    "clevr-act-7-depth_text_aug (rgb   ) L1: 92.865 L1_2d 95.115 px L1_depth: 16.793 cm L1_depth_obj: 13.647 cm cam cvla-droid-1of5c-v1-len=200\n",
    "\n",
    "\n",
    "#clevr-act-7-depth_depthaug (depth) L1: 39.406 L1_depth: 8.897 cm L1_depth_obj: 5.356 cm cam\n",
    "#clevr-act-7-depth_depthaug (depth) L1: 16.043 L1_depth: 3.531 cm L1_depth_obj: 2.930 cm cart\n",
    "\n",
    "#clevr-act-7-depth_text_aug (rgb   ) L1: 35.151 L1_depth: 11.916 cm L1_depth_obj: 7.575 cm cam\n",
    "#clevr-act-7-depth_text_aug (rgb   ) L1: 17.952 L1_depth: 4.256 cm L1_depth_obj: 3.442 cm cart\n",
    "\n",
    "\n",
    "\n",
    "# RGB Networks\n",
    "# Valid Samples: 157 L1: 35.67 (no-agug)\n",
    "# Valid Samples: 160 L1: 34.35 (prompt cleaning) 33.89\n",
    "# Valid Samples: 160 L1: 34.62 (prompt cleaning + simplify text)  -- similar\n",
    "\n",
    "# Valid Samples: 160 L1: 24.96 (rgb20 = augmentation + random background (CVPR09-dataset))\n",
    "# Valid Samples: 160 L1: 26.61 (rgb20 + simplify text) -- worse\n",
    "# Valid Samples: 150 L1: 40.55 (augmentation + DROID background)  -- way worse\n",
    "\n",
    "# With depth (only 31 samples for real data)\n",
    "# Valid Samples: 31 L1: 29.24 (no text aug, NaN-to-Max eval)\n",
    "# Valid Samples: 31 L1: 28.58  L1_depth 6.29375 (no text aug, Nan-to-Min eval)  \n",
    "# Valid Samples: 160 L1: 27.93 6.29375 (no text aug, all-zero)\n",
    "\n",
    "# clevr-act-7-depth_text_aug (rgb  ) L1: 13.994 L1_depth: 11.916 L1_depth_obj: 7.575\n",
    "# clevr-act-7-depth_depthaug (depth) L1: 14.677 L1_depth: 8.897 L1_depth_obj: 5.356 \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.array([13.68125, 48.0125 ,  8.025  ,  8.475  , 20.1875 , 29.4    ,  # without masking v3\n",
    "        5.2875 ,  4.00625, 15.20625,  8.475  , 20.1875 , 29.4    ])\n",
    "\n",
    "np.array([ 5.76875, 14.11875,  7.275  , 36.9875 , 53.45   , 18.25625,  # with masking v2 (wrong orientation)\n",
    "        8.94375,  7.0375 , 16.825  , 36.9875 , 53.45   , 18.25625])\n",
    "\n",
    "\n",
    "\n",
    "eval_on_blocks = {\"baseline\":35.67, \"+clean prompt\": 33.89, \"(simplify text)\":34.62,\n",
    "                  \"rbg-20%\": 24.96, \"(droid bg)\": 40.55, \"text aug\": 25.93}\n",
    "\n",
    "\n",
    "# Extract labels and values\n",
    "labels, values = zip(*eval_on_blocks.items())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Formatting\n",
    "plt.ylabel(\"L1 Error Mean\")\n",
    "plt.title(\"Evaluation over Training Data\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "plt.xlabel(\"Experiment Runs\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)  # Add grid lines for readability\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(valid_diffs[:,:,0:3].reshape(-1,6)))\n",
    "worst_position = np.argsort(-np.max(np.abs(valid_diffs[:,:,2:3]),axis=(1,2)))\n",
    "\n",
    "html_imgs = \"\"\n",
    "for i in range(5):\n",
    "    dataset_i = worst_position[i]\n",
    "    batch_entry = test_dataset[dataset_i]\n",
    "    if return_depth:\n",
    "        (depth, image), sample = batch_entry\n",
    "    else:\n",
    "        image, sample = batch_entry\n",
    "\n",
    "    decoded_str = predictions[dataset_i]\n",
    "    html_img = render_example(image, text=sample[\"prefix\"], label=sample[\"suffix\"], prediction=decoded_str, camera=sample[\"camera\"])\n",
    "    html_imgs += html_img\n",
    "\n",
    "\n",
    "display(HTML(html_imgs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from cvla.hf_model_class import cVLA_wrapped\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_loader_paired import PairedDataset\n",
    "from cvla.utils_eval import Evaluator\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "if username == \"bratulic\":\n",
    "    model_root = Path(\"/work/dlclarge2/bratulic-cvla/models/clevr-act-7-depth_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_nexEnc_RealEval_baseline_augs_2025-04-11_15-52-11\")\n",
    "    model_path = model_root / \"checkpoint-1750\"\n",
    "\n",
    "    model_root2 = Path(\"/work/dlclarge2/bratulic-cvla/models/clevr-act-7-depth_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_NexEnc_augs_2025-04-10_16-32-05\")\n",
    "    model_path2 = model_root2 / \"checkpoint-1200\"\n",
    "\n",
    "    model_root3 = Path(\"/work/dlclarge2/bratulic-cvla/models/clevr-act-7-depth_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_nexEnc_RealEval_baseline_pc25_psim25_CameraPos_augs_2025-04-11_16-03-29\")\n",
    "    model_path3 = model_root3 / \"checkpoint-2000\"\n",
    "\n",
    "    v1 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05debug_2025-04-17_22-56-25\"\n",
    "    v2 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_2025-04-17_23-45-32\"\n",
    "    v3 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_copy025_2025-04-17_23-46-00\"\n",
    "    v4 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_copy025_sort025_cameraSort_2025-04-17_23-45-04\"\n",
    "    v5 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_copy025_sort025_trajSort_2025-04-17_23-47-43\"\n",
    "    v6 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_pBackground02_2025-04-17_23-45-38\"\n",
    "    v7 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_pBackground02_copy025_2025-04-17_23-43-08\"\n",
    "    v8 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_pBackground02_copy025_sort025_cameraSort_2025-04-17_23-46-50\"\n",
    "    v9 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_augs_pBackground02_copy025_sort025_trajSort_2025-04-17_23-48-00\"\n",
    "    v10 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_noAugs_2025-04-17_23-45-28\"\n",
    "    v11 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_noAugs_copy025_2025-04-17_23-45-39\"\n",
    "    v12 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_noAugs_copy025_sort025_cameraSort_2025-04-17_23-44-44\"\n",
    "    v13 = \"cvla-clevr-8_img_1_pr_interleaved_enc_xyzrotvec-cam-512xy128dmaxTokens13_lr3e-05_randomSampling_newData_noAugs_copy025_sort025_trajSort_2025-04-17_23-47-05\"\n",
    "\n",
    "    model_path = Path(\"/work/dlclarge2/bratulic-cvla/models/\") / v2 / \"checkpoint-2250\"\n",
    "\n",
    "elif username == \"argusm\":\n",
    "    model_location = Path(\"/data/lmbraid19/argusm/models/\")\n",
    "    #un = \"_text_lr3e-05xyzrotvec-rbt-256_2025-04-23_13-35-55\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-rbt-128_2025-04-23_13-06-06\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-rbt-100_2025-04-23_12-34-23\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-cam-512xy256d_2025-04-23_12-03-48\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-cam-512xy_2025-04-23_11-04-16\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-cam-1024xy_2025-04-23_11-04-12\"\n",
    "    run = \"_text_lr3e-05xyzrotvec-cam-512xy128d_2025-04-23_11-27-59\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-cam-256xy_2025-04-23_11-04-12\"\n",
    "    #run = \"_text_lr3e-05xyzrotvec-cam-128xy_2025-04-23_11-04-12\"\n",
    "\n",
    "    model_path = model_location / run / \"checkpoint-4687\"\n",
    "\n",
    "model_wrapped = cVLA_wrapped(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = \"text\"\n",
    "if username == \"bratulic\":\n",
    "    conditioning = \"trajectory\"\n",
    "\n",
    "if conditioning == \"trajectory\":\n",
    "    # simulation clevr dataset for testing\n",
    "    dataset_location = Path(\"/tmp/cvla-clevr-8\")\n",
    "    num_images_in_context = 1\n",
    "    image_order = \"interleaved\"\n",
    "    action_encoder = model_wrapped.enc_model.NAME\n",
    "    raw_dataset = H5Dataset(dataset_location, action_encoder=action_encoder)\n",
    "    run_name = f\"_img_{num_images_in_context}_pr_{image_order}_enc_{action_encoder}\"\n",
    "    load_presampled_pairs_path = Path(\"/data/lmbraid21/bratulic/max_pali/datasets\") / f\"cvla-clevr-8_{run_name}_new.pkl\"\n",
    "    presampled_eval_sequences_path = Path(\"/data/lmbraid21/bratulic/max_pali/datasets\") / f\"cvla-clevr-8_{run_name}_pCopy0_pSorting0_presampled_eval_sequences.pkl\"\n",
    "    demonstration_dataset = PairedDataset(raw_dataset, num_images_in_context=num_images_in_context, image_order=image_order, load_presampled_pairs_path=load_presampled_pairs_path, plot_statistics=False,\n",
    "                                p_copy=0.0, p_sort_by_l2_distance=0.0, presampled_path=presampled_eval_sequences_path, mode=\"test\", sort_criteria=\"trajectory_shape\")\n",
    "    model_wrapped.set_conditioning_dataset(demonstration_dataset)\n",
    "    dummy_camera = raw_dataset[0][1][\"camera\"]\n",
    "    evaluator = Evaluator(model_wrapped.enc_model, dummy_camera)\n",
    "else:\n",
    "    dummy_camera = None\n",
    "    evaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mani_skill.examples.run_env import Args, iterate_env\n",
    "\n",
    "parsed_args = Args()\n",
    "parsed_args.obs_mode = \"rgb\"    # for trajs we need only image\n",
    "parsed_args.env_id = \"ClevrMove-v1\"\n",
    "parsed_args.render_mode = \"rgb_array\"\n",
    "parsed_args.control_mode = \"pd_joint_pos\"\n",
    "parsed_args.quiet = True\n",
    "parsed_args.action_encoder = model_wrapped.enc_model.NAME\n",
    "env_iter = iterate_env(parsed_args, vis=False, model=model_wrapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode verbose\n",
    "iters_reached = 0\n",
    "num_iters = 50\n",
    "reward_succes = 0\n",
    "for i in range(num_iters):\n",
    "    output = next(env_iter)\n",
    "    try:\n",
    "        json_dict = output[1]\n",
    "        print(\"TRY \", i, \"Prefix -> Suffix:\", json_dict[\"prefix\"], \" -> \", json_dict[\"suffix\"])\n",
    "        print(\"Prediction with reward:\", json_dict[\"prediction\"], json_dict[\"reward\"])\n",
    "        if evaluator:\n",
    "            evaluator.evaluate(json_dict[\"prediction\"], json_dict[\"suffix\"])\n",
    "        iters_reached = json_dict[\"iter_reached\"]\n",
    "        if json_dict[\"reward\"] > 0.75:\n",
    "            reward_succes += 1\n",
    "    except:\n",
    "        print(len(output))\n",
    "    \n",
    "if evaluator:\n",
    "    stats = evaluator.report_stats()\n",
    "    for metric_name, metric_value in stats.items():\n",
    "        print(f\"{metric_name}: {metric_value:.3f}\")\n",
    "    evaluator.reset()\n",
    "\n",
    "iters_reached += 1  # starts with 0 so we add 1 to it\n",
    "\n",
    "print(f\"Total number of tries {iters_reached} for {num_iters} tries gaves {100*num_iters / iters_reached:.2f}% valid sequence rate.\")\n",
    "print(f\"Reward success rate: {100*reward_succes / num_iters:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demonstration_dataset.task_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
