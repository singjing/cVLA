{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LqvmtZPzyY1"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Fine-tune PaliGemma2 on Object Detection Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md)\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)\n",
    "\n",
    "PaliGemma 2 is built by combining the SigLIP-So400m vision encoder with the more recent and capable language models from the Gemma 2 family.\n",
    "\n",
    "![PaliGemma2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-1.png)\n",
    "\n",
    "The authors use a 3-stage training approach similar to the original PaliGemma. In stage 1, they combine the pretrained vision and language model components and train them jointly on a multimodal task mixture. In stage 2, they train the models at higher resolutions of 448px^2 and 896px^2. In stage 3, they fine-tune the models on the target transfer tasks.\n",
    "\n",
    "PaliGemma 2 models outperform the original PaliGemma at the same resolution and model size. Increasing the model size and resolution generally improves performance across a wide range of tasks, but the benefits differ depending on the task. Some tasks benefit more from increased resolution, while others benefit more from a larger language model.\n",
    "\n",
    "![PaliGemma2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-2.png)\n",
    "\n",
    "Notebook requires A100 with 40GB of VRAM to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBp3Czz3GBmc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADTkh-2y_9Yv"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To fine-tune PaliGemma2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
    "- In Colab, go to the left pane and click on `Secrets` (🔑).\n",
    "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
    "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wyojKiG_hX9"
   },
   "source": [
    "### Select the runtime\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_8BLW6R_x-z",
    "outputId": "678f6080-89ea-41f7-eecb-a592d1a03d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 24 01:31:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:E1:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8             22W /  350W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"   #,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMlw3ru1YvLg"
   },
   "source": [
    "### Download dataset from Roboflow Universe\n",
    "\n",
    "To fine-tune PaliGemma2, prepare your dataset in JSONL format. You can use Roboflow to easily convert any dataset into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtvz4QZ9YuG8",
    "outputId": "8edb179f-f6a1-421d-f50a-2ee61724415a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "sending incremental file list\n",
      "/tmp/indoorCVPR: directory\n",
      "/tmp/training2: directory\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q peft bitsandbytes transformers==4.47.0 tf-keras\n",
    "!rsync -a --progress /data/lmbraid19/argusm/datasets/indoorCVPR_09.tar /tmp/ && mkdir -p /tmp/indoorCVPR && tar -xf /tmp/indoorCVPR_09.tar -C /tmp/indoorCVPR\n",
    "!rsync -a --progress /work/dlclarge2/zhangj-zhangj-CFM/data/training2 /tmp/\n",
    "!file /tmp/indoorCVPR\n",
    "!file /tmp/training2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ElnTKvBVa"
   },
   "source": [
    "**NOTE:** Let's read the first few lines of the annotation file and examine the dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLhSenP5AtQe",
    "outputId": "f6a6b7f0-6360-4eaf-8abf-bd40462af58a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/sapien/_vulkan_tricks.py:37: UserWarning: Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\n",
      "  warn(\"Failed to find system libvulkan. Fallback to SAPIEN builtin libvulkan.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 88244 | Train: 87244 | Val: 1000| Smallv:200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = H5Dataset(dataset_location, augment_rgb=augment_image_rgb, augment_text=complexify_text,\\n                          augment_depth=augment_depth, return_depth=True,action_encoder=\"xyzrotvec-cam-512xy\")\\n#, augment_rgbds=randomize_background\\n\\nprint(\"dataset_location:\", dataset_location,\"samples:\", len(train_dataset))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from cvla.data_loader_h5 import H5Dataset\n",
    "from cvla.data_loader_jsonl import JSONLDataset\n",
    "from cvla.data_augmentations import augment_image_rgb, RandomizeBackgrounds\n",
    "from cvla.data_augmentations import complexify_text, DepthAugmentation\n",
    "from cvla.data_loader_images import ImageFolderDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import random\n",
    "\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "dataset_location = Path(\"/tmp/training2\")\n",
    "\n",
    "bg_image_dataset = ImageFolderDataset(\"/tmp/indoorCVPR/Images\", transform=transforms.RandomResizedCrop((448,448)))\n",
    "randomize_background = RandomizeBackgrounds(p=0.2, background_images=bg_image_dataset)\n",
    "augment_depth = DepthAugmentation(depth_range=(25, 100), max_delta_depth=35)\n",
    "\n",
    "full_dataset = H5Dataset(\n",
    "    dataset_location,\n",
    "    augment_rgb=augment_image_rgb,\n",
    "    augment_text=complexify_text,\n",
    "    augment_depth=augment_depth,\n",
    "    return_depth=False,\n",
    "    action_encoder=\"xyzrotvec-cam-512xy\",\n",
    ")\n",
    "\n",
    "# 手动定义验证集大小\n",
    "val_size = 1000  # 固定1000条\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "val_indices_small = random.sample(range(len(val_dataset)), 200)\n",
    "val_dataset_small = torch.utils.data.Subset(val_dataset, val_indices_small)\n",
    "\n",
    "print(f\"Total samples: {len(full_dataset)} | Train: {len(train_dataset)} | Val: {len(val_dataset)}| Smallv:{len(val_dataset_small)}\")\n",
    "\n",
    "\n",
    "'''\n",
    "train_dataset = H5Dataset(dataset_location, augment_rgb=augment_image_rgb, augment_text=complexify_text,\n",
    "                          augment_depth=augment_depth, return_depth=True,action_encoder=\"xyzrotvec-cam-512xy\")\n",
    "#, augment_rgbds=randomize_background\n",
    "\n",
    "print(\"dataset_location:\", dataset_location,\"samples:\", len(train_dataset))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McU9159EvkeA"
   },
   "source": [
    "### Set up and test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "19pQQjIixL-T",
    "outputId": "c59a8054-4f92-4cb0-b1cb-6a7b89e6e4b6"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "image was <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     image, sample = train_dataset[i]\n\u001b[32m     10\u001b[39m     prefix = sample[\u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     html_imgs += \u001b[43mrender_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcamera\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m plot_images = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m plot_images:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/utils_vis.py:124\u001b[39m, in \u001b[36mrender_example\u001b[39m\u001b[34m(image, label, prediction, camera, enc, enc_pred, text, i, extra_text, draw_state_coords, draw_pred_coords, draw_label_coords)\u001b[39m\n\u001b[32m    122\u001b[39m     image_width, image_height, _ = image.shape\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimage was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m camera \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: getting standard camera\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: image was <class 'list'>"
     ]
    }
   ],
   "source": [
    "from cvla.utils_vis import render_example\n",
    "import matplotlib.pyplot as plt\n",
    "from cvla.utils_traj_tokens import getActionEncInstance\n",
    "\n",
    "enc = getActionEncInstance(\"xyzrotvec-cam-512xy\")\n",
    "num_samples = 3*2\n",
    "html_imgs = \"\"\n",
    "for i in range(num_samples):\n",
    "    image, sample = train_dataset[i]\n",
    "    prefix = sample[\"prefix\"]\n",
    "    html_imgs += render_example(image, label=sample[\"suffix\"], enc=enc, text=prefix, camera=sample[\"camera\"])\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvYNxYbBtE3"
   },
   "source": [
    "### Load PaliGemma2 model\n",
    "\n",
    "**NOTE:** PaliGemma2 offers 9 pre-trained models with sizes of `3B`, `10B`, and `28B` parameters, and resolutions of `224`, `448`, and `896` pixels. In this tutorial, I'll be using the [`google/paligemma2-3b-pt-448`](https://huggingface.co/google/paligemma2-3b-pt-448) checkpoint. Resolution has a key impact on the mAP of the trained model, and it seems that `448` offers the most optimal balance between performance and compute resources required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "a5cd856a1fbf413a9aa4edff7cf3e636",
      "23be9405ad7f4cd8817db36f2f3834dd",
      "2e2e11bda8024f97868d45598bff6dc8",
      "11a966b75e124f369844afda5db25ab6",
      "37650cf71172405392ab843fd82360c3",
      "4fc0f4ab93ff4fddb482a8d6820b1253",
      "cda1f28093f04b1ca2e3689d1e314e10",
      "972c99a291d24b9f96ea09df63d5cf4b",
      "83aa606372b848659e13d683fa2fe09e",
      "3c26b58a776346f2920f810d99b21545",
      "fc9c7ae18f7341598a741ee8811896ff",
      "632c3fbb8739429b875687049eb1cf61",
      "7aeec69c06b049738c1fae480103cb1d",
      "c4e5a6e67ec3443fb73bfa86a27df704",
      "9355cb34cea948909504c1144169b693",
      "7a294c9159804eb388a2d7fe67f2920e",
      "7877bed5b8144f78a5a3f8b33741a82b",
      "dcb6f37a97664ed8ad2a699956bd772b",
      "ee2fa3db801344ba8564f700ca5ee2c5",
      "d5b373e9c1fc4d078e5512764235c6ba"
     ]
    },
    "id": "VfCgxIp3EjmC",
    "outputId": "db59a5d0-440f-4ca3-c5a3-194b9e22f7bd"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2f877f70e9b241f08717bba2b7616198",
      "9393bb98f83e4bf1bd8becdfdfb41f12",
      "41a50a96ca224e0aa12ef142d317e762",
      "cfcdcf908ff04fa19159c98b08c851ce",
      "9165a3ed1f2d4671b089dd00cae0fdbb",
      "6873590d9ac9436facef6cb182e1c077",
      "7ef24d8b5077400b9e005b373ab8d4f1",
      "df5fb754518547b480d95ce715bcdd8b",
      "c3eddb53526e4694bbf5ee3636e1b806",
      "eaf748f0d17148eba7e6351d107c1a50",
      "625fb0f94f5546e0a643192c2bd6c40f",
      "45db0bb8841f48e38c8eb8d28c1e72b8",
      "254a4ab182234215a7aeb6873f6c62af",
      "b7c2a5216879471c982406086571af7e",
      "9db3a6d6cd9548878d0998150f897ff1",
      "3226844ff08a46dcbc982432456d496b",
      "94f8f306b995471291fc610c4f170b26",
      "b91043be62ef4965852d9dcf5faf8edc",
      "141bc95f2dcc485cb9d23f0f01d28f24",
      "a770c3f09c204bf7b879a0d72256977d",
      "545d3d664c8e49cab3270012be39a604",
      "eeb274cb895f416bb85211d8ed07c005",
      "a3be6c18dd6440e48e80a9dc70bf9c9a",
      "f6619c93a7274805bb3005b0898d5f5f",
      "ef7ffed4ac344f7aaf1f39417284f4e3",
      "eab62ec555b146f4802630f6dd38afc6",
      "3dd89974f8b74268bf295119037ab0b8",
      "b18537a50e8f4e4d9829735ea2a98596",
      "d7f77f6a75f9441b8b1231b2eec3123a",
      "9f36c016c2a24d398e84c62964c47277",
      "dce11ec58ce84d9ea0ae3ac26519e796",
      "1a33f188e7ce499ebcb96e7e1617efe4",
      "be391a3e32d04aea837517041123cece",
      "4dd7115fec054f4790524d469bb263f7",
      "b6a7688a29f74e18b9c10ca185585457",
      "ed85f01229c146a48ae86370c36fb588",
      "2e5cf178fc03434faad8539094ddce4a",
      "ae8a2d84dd28458ba68bc94553337064",
      "20f03fcb438749f5a95cd3a72246fafd",
      "6f9622e6bdb1478e99ddddd1e2257f01",
      "f6e321f3c19f41688f3b2fd84243e851",
      "52d311c2b3af4979b433a392e77b5b21",
      "bddb4ea70dab445badbd5a4b2ef66ed4",
      "e9eb6315ff8a42f4911a3cee35e8320b"
     ]
    },
    "id": "ntXj4A3SyEAa",
    "outputId": "898b4343-0089-4ee6-8a54-d1f76d39b59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda visible devices: 0,1\n",
      "cuda\n",
      "Using device: cuda\n",
      "Good devices [0, 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2074262201ee4211960859dd907234b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "#transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "print(\"cuda visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "devices_good = sorted((int(x) for x in os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")))\n",
    "DEVICE = torch.device('cuda')\n",
    "print(DEVICE)\n",
    "print('Using device:', DEVICE)\n",
    "print(\"Good devices\", devices_good)\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "# use checkpoint\n",
    "#LOCAL_CHECKPOINT = \"/data/lmbraid19/argusm/models/_text_lr3e-05xyzrotvec-cam-512xy256d_2025-04-23_12-03-48/checkpoint-4687\"\n",
    "\n",
    "#fine-tune directly on paligemma2\n",
    "MODEL_NAME = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "processor = PaliGemmaProcessor.from_pretrained(\"google/paligemma2-3b-pt-224\")\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    #LOCAL_CHECKPOINT,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=None,\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "#.to(\"cuda\") \n",
    "tokenizer = processor.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the red folder in the red ball <loc0271><loc0331><loc0049><seg045><seg087><seg094>', 'pick up the pyramid shaped keycap and put it inside the cylindrical dark flashlight <loc0171><loc0049><loc0036><seg048><seg080><seg094>', 'put the fiddler crab inside the conical seashell <loc0199><loc0353><loc0054><seg051><seg084><seg101>']\n",
      "input_ids torch.Size([3, 549])\n",
      "token_type_ids torch.Size([3, 549])\n",
      "attention_mask torch.Size([3, 549])\n",
      "pixel_values torch.Size([6, 3, 224, 224])\n",
      "labels torch.Size([3, 549])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "#debug\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)#.to(DEVICE)\n",
    "    print(\"prefixes\", prefixes)\n",
    "    return inputs\n",
    "\n",
    "batch = [train_dataset[i] for i in range(3)]\n",
    "inputs = collate_fn(batch)\n",
    "for x in inputs:\n",
    "    print(x, inputs[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,      0,      0,  ..., 257106, 257111,      1],\n",
       "        [257152, 257152, 257152,  ..., 257115, 257139,      1],\n",
       "        [     0,      0,      0,  ..., 257116, 257138,      1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_path /work/dlclarge2/zhangj-zhangj-CFM/models/training2test\n",
      "TRAIN_STEPS 2726\n",
      "GRAD_ACCUM 16\n"
     ]
    }
   ],
   "source": [
    "from cvla.utils_eval import Evaluator\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_EXAMPLES = len(train_dataset)\n",
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_DEV = 2\n",
    "GRAD_ACCUM = int(round(BATCH_SIZE / BATCH_SIZE_DEV))\n",
    "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "SEQLEN = 12\n",
    "#EVAL_STEPS = 200\n",
    "EVAL_STEPS = 2\n",
    "SAVE_LIMIT = 5\n",
    "LOGGING_STEPS = 10\n",
    "\n",
    "\n",
    "run_name = \"test\"\n",
    "new_model_location = Path(\"/work/dlclarge2/zhangj-zhangj-CFM/models\")\n",
    "save_path = new_model_location / (str(Path(dataset_location).stem) + run_name)\n",
    "print(\"save_path\", save_path)\n",
    "print(\"TRAIN_STEPS\",TRAIN_STEPS)\n",
    "print(\"GRAD_ACCUM\", GRAD_ACCUM)\n",
    "\n",
    "writer = SummaryWriter(log_dir=str(save_path / \"tb_logs\"))\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"\n",
    "    Trainer that:\n",
    "      - uses normal loss for training\n",
    "      - runs model.generate() for evaluation\n",
    "      - uses Evaluator to compute real-world metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "    \n",
    "        outputs = model(**inputs)\n",
    "        loss = getattr(outputs, \"loss\", None)\n",
    "        if loss is None:\n",
    "            raise ValueError(\"Model outputs do not contain a 'loss' field.\")\n",
    "\n",
    "        if self.state.global_step % self.args.logging_steps == 0:\n",
    "            writer.add_scalar(\"train/loss_total\", loss.item(), self.state.global_step)\n",
    "            writer.add_scalar(\"train/lr\", self.optimizer.param_groups[0][\"lr\"], self.state.global_step)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"\n",
    "        Overridden evaluation that generates predictions textually\n",
    "        and computes spatial metrics via Evaluator.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dataset = eval_dataset or self.eval_dataset\n",
    "        # helper: unwrap nested Subsets to access H5Dataset\n",
    "        def unwrap_dataset(dset):\n",
    "            while hasattr(dset, \"dataset\"):\n",
    "                dset = dset.dataset\n",
    "            return dset\n",
    "\n",
    "        base_dataset = unwrap_dataset(dataset)\n",
    "        camera = dataset[0][1][\"camera\"]\n",
    "\n",
    "        evaluator = Evaluator(\n",
    "            getActionEncInstance(\"xyzrotvec-cam-512xy\"),\n",
    "            camera_fixed=camera,\n",
    "            encoder_labels=base_dataset.action_encoder,  # ✅ now always valid\n",
    "        )\n",
    "        # sample limited subset for speed\n",
    "        eval_batch_size = self.args.per_device_eval_batch_size\n",
    "        test_samples = min(len(dataset), 200)\n",
    "        device = next(self.model.parameters()).device\n",
    "        \n",
    "        for start_idx in tqdm(range(0, test_samples, eval_batch_size), total=ceil(test_samples / eval_batch_size)):\n",
    "            batch_i = range(start_idx, min(start_idx + eval_batch_size, test_samples))\n",
    "            batch = [dataset[i] for i in batch_i]\n",
    "            inputs = self.data_collator(batch)\n",
    "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            prefix_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                generation = self.model.generate(**inputs, max_new_tokens=13, do_sample=False, use_cache=False)\n",
    "                decoded = [\n",
    "                    self.processing_class.decode(x[prefix_length:], skip_special_tokens=True) for x in generation\n",
    "                ]\n",
    "                decoded_labels = [\n",
    "                    self.processing_class.decode([t for t in x.tolist() if t >= 0], skip_special_tokens=True)\n",
    "                    for x in inputs[\"labels\"]\n",
    "                ]\n",
    "            if start_idx == 0:\n",
    "                print(\"decoded[0]:\", decoded[0] if decoded else None)\n",
    "                print(\"decoded_label[0]:\", decoded_labels[0] if decoded_labels else None)\n",
    "\n",
    "            for pred, label in zip(decoded, decoded_labels):\n",
    "                evaluator.evaluate(pred, label, camera=camera)\n",
    "\n",
    "        stats = evaluator.report_stats()\n",
    "        metrics = {f\"{metric_key_prefix}_{k}\": v for k, v in stats.items()}\n",
    "\n",
    "        # log to TensorBoard\n",
    "        for k, v in metrics.items():\n",
    "            writer.add_scalar(k, v, self.state.global_step)\n",
    "\n",
    "        self.log(metrics)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with JAX settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_708122/1012045069.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        if \"self_attn\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "args_jax = Seq2SeqTrainingArguments(\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE_DEV,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=3e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    generation_max_length=SEQLEN,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    optim=\"adafactor\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=EVAL_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cart_l1\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    "    output_dir=save_path,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset_small,\n",
    "    data_collator=collate_fn,  # replace with your collate_fn if you use one\n",
    "    args=args_jax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only when recover from last time training\n",
    "#last_checkpoint = \"/work/dlclarge2/zhangj-zhangj-CFM/models/training2_topview_70000_based/checkpoint-183\"\n",
    "#trainer.train(resume_from_checkpoint=last_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixesprefixes  ['put the glossy smooth mango in the curved object <loc0323><loc0086><loc0046><seg056><seg091><seg109>', 'put the compact gray spacecraft inside the turquoise model <loc0252><loc0266><loc0047><seg045><seg089><seg094>']['put the cursive nameplate in the ring-shaped chocolate pastry <loc0242><loc0169><loc0054><seg044><seg090><seg095>', 'put the blocky blue figure inside the floral porcelain teacup <loc0288><loc0250><loc0056><seg049><seg090><seg100>']prefixes\n",
      " \n",
      "['put the vintage brass candleholder in the ceramic bottle stopper <loc0214><loc0253><loc0048><seg063><seg098><seg115>', 'put the donut in the aluminum can <loc0242><loc0421><loc0053><seg045><seg086><seg094>']prefixes\n",
      " ['put the diverse succulent mix in the off-white bone shape <loc0223><loc0456><loc0037><seg052><seg084><seg102>', 'put the glossy blue orb in the irregular stone-like slab <loc0301><loc0511><loc0030><seg041><seg067><seg070>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixesprefixes  ['put the cooling fan inside the realistic button mushroom <loc0002><loc0081><loc0037><seg060><seg049><seg011>', 'put the sleek maroon car in the cartoonish red tomato <loc0260><loc0511><loc0040><seg040><seg071><seg075>']['put the shiny golden ornament in the rustic metal mug <loc0246><loc0511><loc0032><seg039><seg070><seg072>', 'put the flaky almond croissant in the mandible <loc0302><loc0065><loc0046><seg049><seg084><seg098>']\n",
      "\n",
      "prefixes ['put the blue ornament inside the clownfish <loc0183><loc0511><loc0034><seg047><seg062><seg060>', 'put the rectangular crumpled package in the colorful model <loc0275><loc0407><loc0051><seg040><seg074><seg078>']\n",
      "prefixes ['put the crenellated chess piece in the metal rods <loc0295><loc0374><loc0049><seg045><seg086><seg094>', 'put the glossy brown donut inside the pixelated pink glasses <loc0235><loc0368><loc0041><seg040><seg072><seg076>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the ghostly toy figure in the model cicada <loc0293><loc0069><loc0045><seg046><seg081><seg092>', 'put the cube in the red cube apple <loc0191><loc0262><loc0038><seg047><seg078><seg089>']prefixes\n",
      " ['place the orange-yellow can design in the circular base terrain <loc0233><loc0261><loc0052><seg050><seg089><seg102>', 'put the 20-sided red die in the vintage toy car <loc0218><loc0163><loc0054><seg043><seg081><seg087>']\n",
      "prefixes ['put the aged crescent pendant in the mineral <loc0171><loc0326><loc0051><seg048><seg066><seg072>', 'put the white box inside the colorful dna structure <loc0249><loc0086><loc0040><seg045><seg070><seg079>']\n",
      "prefixes ['pick the red bracket and put it in the yellow toy car <loc0232><loc0340><loc0042><seg050><seg083><seg099>', 'pick the cartoon head and put it in the jawbone <loc0152><loc0269><loc0048><seg057><seg039><seg016>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the pink character in the dark bowl spoon <loc0291><loc0009><loc0044><seg059><seg099><seg111>', 'put the bumpy cherimoya fruit in the mineral <loc0085><loc0022><loc0043><seg066><seg039><seg006>']\n",
      "prefixes ['put the cylindrical bullet in the purple heart <loc0233><loc0262><loc0052><seg048><seg092><seg099>', 'put the classic light bulb in the mickey shapes <loc0277><loc0115><loc0052><seg055><seg097><seg108>']\n",
      "prefixes ['put the turquoise mug in the iron <loc0269><loc0099><loc0050><seg053><seg092><seg105>', 'put the wireless black mouse in the brown paper cup <loc0069><loc0277><loc0042><seg051><seg065><seg064>']\n",
      "prefixes ['put the textured greenish-yellow melon in the lifelike crab model <loc0268><loc0152><loc0053><seg055><seg095><seg109>', 'pick the spherical red gray and put it in the yellow frame sunglasses <loc0226><loc0026><loc0048><seg061><seg101><seg113>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the brown seashell in the topographic model <loc0199><loc0254><loc0045><seg046><seg086><seg094>', 'put the molecular model display in the rectangular brown platform <loc0254><loc0488><loc0043><seg042><seg082><seg088>']\n",
      "prefixes ['put the realistic brown pinecone inside the tartan mug <loc0097><loc0332><loc0042><seg043><seg060><seg055>', 'place the industrial check valve inside the yellow bat <loc0139><loc0093><loc0037><seg050><seg057><seg047>']\n",
      "prefixes ['put the textured bird figure in the colorful caps <loc0267><loc0169><loc0053><seg045><seg078><seg086>', 'put the choker with heart in the square white mug <loc0187><loc0259><loc0053><seg058><seg039><seg016>']\n",
      "prefixes ['put the temple model in the red mushroom <loc0303><loc0441><loc0043><seg040><seg065><seg067>', 'put the modern t-shaped pipe in the blue bracket <loc0040><loc0181><loc0048><seg053><seg057><seg045>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes prefixes['put the rusty metal nut inside the small beige gazebo <loc0274><loc0243><loc0062><seg046><seg080><seg088>', 'put the small bird model in the model insect <loc0232><loc0110><loc0046><seg056><seg095><seg110>'] \n",
      "['pick up the faceted magenta ball and put it in the stylized black bat <loc0175><loc0388><loc0039><seg042><seg059><seg057>', 'put the gold dual-ring structure inside the small gold container <loc0291><loc0001><loc0040><seg061><seg100><seg111>']prefixes\n",
      " ['put the bone replica in the gray beige fossil <loc0268><loc0511><loc0032><seg045><seg070><seg077>', 'put the vibrant green pear in the mug <loc0244><loc0482><loc0038><seg051><seg089><seg102>']prefixes\n",
      " ['put the creamy dessert cup in the natural decorative piece <loc0148><loc0511><loc0034><seg046><seg065><seg065>', 'put the roll of tape in the stylized beige rook <loc0247><loc0167><loc0048><seg062><seg026><seg015>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the miniature house in the ceramic mug <loc0255><loc0119><loc0046><seg051><seg094><seg102>', 'put the smooth wooden block in the thin oval lenses <loc0228><loc0463><loc0051><seg039><seg072><seg074>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2726' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2726 00:06 < 4:46:34, 0.16 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['pick the sleek classic stopwatch and put it in the colorful location map <loc0266><loc0347><loc0050><seg045><seg078><seg087>', 'put the weathered horseshoe decor in the plush round cushion <loc0183><loc0317><loc0054><seg041><seg074><seg078>']\n",
      "prefixes ['put the broccoli in the deodorant <loc0247><loc0511><loc0038><seg044><seg064><seg063>', 'put the dome-shaped wasp nest in the metal bracket <loc0250><loc0303><loc0052><seg052><seg092><seg104>']\n",
      "prefixes ['put the sleek light bulb inside the textured oval dish <loc0230><loc0421><loc0045><seg043><seg081><seg089>', 'put the tarnished metallic goblet in the red cap character <loc0249><loc0344><loc0054><seg045><seg089><seg095>']\n",
      "prefixes ['put the metal skull inside the realistic heart model <loc0193><loc0150><loc0041><seg053><seg092><seg106>', 'put the spiraled conical shell in the smooth red disc <loc0303><loc0315><loc0040><seg039><seg068><seg071>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixesprefixes prefixes  ['place the marbled clutch bag in the ear <loc0309><loc0047><loc0048><seg054><seg090><seg108>', 'put the bulb in the pentagonal mug <loc0190><loc0084><loc0049><seg050><seg078><seg094>']['put the glossy red sphere in the olympus camera <loc0254><loc0396><loc0043><seg036><seg074><seg074>', 'put the twisted cork piece in the cube <loc0238><loc0229><loc0041><seg048><seg091><seg100>']['put the pixelated figure in the mouth model <loc0161><loc0511><loc0036><seg051><seg065><seg068>', 'put the gray chunk in the pink airplane <loc0258><loc0364><loc0056><seg042><seg074><seg079>']\n",
      "\n",
      "\n",
      "prefixes ['put the cheesecake in the donut <loc0177><loc0252><loc0058><seg049><seg078><seg094>', 'put the yellow bumpy fruit inside the gameboy <loc0298><loc0001><loc0032><seg045><seg082><seg092>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes prefixes['put the green calcite inside the rectangular support <loc0279><loc0278><loc0055><seg061><seg028><seg013>', 'put the emoji inside the shoe <loc0245><loc0190><loc0051><seg049><seg093><seg102>'] \n",
      "['pick up the vintage red truck and put it in the ice cream <loc0031><loc0511><loc0041><seg050><seg063><seg062>', 'put the u-shaped magnet inside the handmade clay dish <loc0320><loc0344><loc0046><seg040><seg069><seg073>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the switch in the green ball <loc0293><loc0375><loc0040><seg041><seg076><seg081>', 'put the flat solar panel in the pastry <loc0255><loc0459><loc0037><seg042><seg081><seg087>']\n",
      "prefixes ['put the green head inside the small electronic board <loc0154><loc0302><loc0047><seg042><seg071><seg077>', 'put the apple into the grenade <loc0284><loc0250><loc0047><seg046><seg088><seg096>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the rectangular electrical connector inside the cartoonish purple figure <loc0248><loc0133><loc0053><seg038><seg077><seg081>', 'put the red can in the wall outlet <loc0091><loc0506><loc0048><seg047><seg061><seg054>']\n",
      "prefixesprefixes  ['put the small green surface inside circular metal dial <loc0178><loc0208><loc0046><seg047><seg082><seg094>', 'put the rectangular black box in the fossilized shell object <loc0284><loc0378><loc0040><seg047><seg087><seg096>']['put the white bottle in the decorative teal fish <loc0244><loc0347><loc0045><seg039><seg069><seg071>', 'pick the orange soda can and put it in the square pink plate <loc0253><loc0254><loc0059><seg056><seg095><seg111>']\n",
      "\n",
      "prefixes ['put the glass cup in the gold hexagonal nut <loc0198><loc0279><loc0059><seg044><seg058><seg052>', 'put the pale yellow seashell in the curved yellow teapot <loc0221><loc0351><loc0049><seg046><seg087><seg096>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                              | 0/25 [00:00<?, ?it/s]You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/logging/__init__.py\", line 1160, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/logging/__init__.py\", line 999, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/logging/__init__.py\", line 703, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/logging/__init__.py\", line 392, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_708122/3456323512.py\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py\", line 2598, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py\", line 3071, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py\", line 3025, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "  File \"/tmp/ipykernel_708122/919551171.py\", line 87, in evaluate\n",
      "    generation = self.model.generate(**inputs, max_new_tokens=13, do_sample=False, use_cache=False)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2011, in generate\n",
      "    generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/generation/utils.py\", line 1545, in _prepare_generation_config\n",
      "    model_kwargs = generation_config.update(**kwargs)\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/generation/configuration_utils.py\", line 1325, in update\n",
      "    self.validate()\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/generation/configuration_utils.py\", line 789, in validate\n",
      "    logger.warning_once(\n",
      "  File \"/home/zhangj/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/utils/logging.py\", line 328, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the gauge in the bright pink yarn <loc0105><loc0073><loc0035><seg049><seg081><seg096>', 'put the circular wristwatch case in the blue pineapple carton <loc0292><loc0357><loc0042><seg044><seg084><seg092>', 'put the military compass in the crumpled paper <loc0259><loc0180><loc0056><seg049><seg090><seg100>', 'put the orange cartoon cat inside the glass with beverage <loc0265><loc0266><loc0053><seg039><seg065><seg067>', 'put the macaron inside the compact red tape <loc0239><loc0367><loc0060><seg043><seg069><seg075>', 'pick up the rectangular floral box and put it in the compact black revolver <loc0149><loc0185><loc0047><seg052><seg078><seg097>', 'put the eyeball in the fish shaped lamp <loc0178><loc0389><loc0050><seg042><seg076><seg082>', 'put the dark planter in the mechanical keyboard <loc0251><loc0252><loc0056><seg055><seg092><seg108>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▍                                                                                  | 1/25 [00:06<02:29,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded[0]: the book is on the floor\n",
      "decoded_label[0]: <loc0106><loc0180><loc0069><seg047><seg077><seg089><loc0209><loc0001><loc0031><seg047><seg077><seg089>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the l-shaped red bracket in the matte black can <loc0173><loc0331><loc0041><seg056><seg087><seg110>', 'put the stylized eye model in the seashell <loc0280><loc0355><loc0057><seg045><seg078><seg089>', 'put the green toy in the glossy red brick <loc0165><loc0294><loc0057><seg047><seg057><seg052>', 'put the rustic wooden mug in the sugar bowl <loc0274><loc0335><loc0057><seg041><seg069><seg074>', 'put the rustic brown mug in the caliper <loc0250><loc0182><loc0045><seg051><seg091><seg103>', 'put the cartoonish head inside the red strap flip-flops <loc0267><loc0011><loc0055><seg058><seg035><seg013>', 'put the marbled blue dice in the textured white wedge <loc0276><loc0136><loc0057><seg048><seg084><seg098>', 'put the elegant martini glass in the red mushroom <loc0276><loc0156><loc0052><seg062><seg028><seg011>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▉                                                                               | 2/25 [00:11<02:09,  5.63s/it]You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefixes ['put the rectangular eyeglasses design inside the model wasp <loc0274><loc0153><loc0049><seg061><seg029><seg012>', 'place the cube inside the pixelated figure <loc0260><loc0394><loc0055><seg037><seg072><seg073>', 'put the textured brown object in the textured celestial body <loc0190><loc0329><loc0045><seg043><seg076><seg083>', 'place the white coral inside the rectangular dark eyeglasses <loc0204><loc0099><loc0052><seg056><seg035><seg019>', 'put the pink teapot inside the brown rocky terrain <loc0246><loc0316><loc0054><seg037><seg068><seg070>', 'put the sporty modern sunglasses in the compact modern drone <loc0168><loc0056><loc0041><seg050><seg053><seg038>', 'put the frog in the apple <loc0240><loc0342><loc0061><seg058><seg091><seg111>', 'put the polygonal rustic mug into the pink shoe tiles <loc0238><loc0223><loc0047><seg046><seg054><seg046>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▎                                                                           | 3/25 [00:16<01:58,  5.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m trainer.save_model(\u001b[38;5;28mstr\u001b[39m(save_path / \u001b[33m\"\u001b[39m\u001b[33mfinal_checkpoint\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      3\u001b[39m writer.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:2598\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2596\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2597\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2598\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2602\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:3071\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3069\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3070\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3072\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/transformers/trainer.py:3025\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3028\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mCustomTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, test_samples, eval_batch_size), total=ceil(test_samples / eval_batch_size)):\n\u001b[32m     80\u001b[39m     batch_i = \u001b[38;5;28mrange\u001b[39m(start_idx, \u001b[38;5;28mmin\u001b[39m(start_idx + eval_batch_size, test_samples))\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     batch = [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_i]\n\u001b[32m     82\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.data_collator(batch)\n\u001b[32m     83\u001b[39m     inputs = {k: v.to(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/utils/data/dataset.py:408\u001b[39m, in \u001b[36mSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset[[\u001b[38;5;28mself\u001b[39m.indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/torch/utils/data/dataset.py:408\u001b[39m, in \u001b[36mSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset[[\u001b[38;5;28mself\u001b[39m.indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/data_loader_h5.py:71\u001b[39m, in \u001b[36mH5Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m retry_count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries):\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetitem_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     73\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOSError encountered for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Retrying in 3 seconds...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/data_loader_h5.py:104\u001b[39m, in \u001b[36mH5Dataset.getitem_func\u001b[39m\u001b[34m(self, idx, force_augs)\u001b[39m\n\u001b[32m    101\u001b[39m camera_intrinsic = \u001b[38;5;28mself\u001b[39m.h5_file[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraj_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/obs/sensor_param/render_camera/intrinsic_cv\u001b[39m\u001b[33m\"\u001b[39m][frame_idx]\n\u001b[32m    102\u001b[39m camera_extrinsic = \u001b[38;5;28mself\u001b[39m.h5_file[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraj_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/obs/sensor_param/render_camera/extrinsic_cv\u001b[39m\u001b[33m\"\u001b[39m][frame_idx]\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mh5_file\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraj_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/obs/sensor_data/render_camera/rgb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m#top = self.h5_file[f'traj_{idx}/obs/sensor_data/top_camera/rgb'][0]\u001b[39;00m\n\u001b[32m    106\u001b[39m sensor_data = \u001b[38;5;28mself\u001b[39m.h5_file[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraj_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/obs/sensor_data\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mani_skill_clean/lib/python3.12/site-packages/h5py/_hl/dataset.py:820\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(str(save_path / \"final_checkpoint\"))\n",
    "writer.close()\n",
    "print(\"✅ Training completed successfully with Evaluator-based validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in inputs.items():\n",
    "    if torch.is_tensor(value):\n",
    "        inputs[key] = value.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "for k, v in inputs.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"  {k}: {v.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)\n",
    "print({k: v.device for k, v in inputs.items() if torch.is_tensor(v)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
