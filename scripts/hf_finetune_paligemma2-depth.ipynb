{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LqvmtZPzyY1"
   },
   "source": [
    "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
    "\n",
    "# Fine-tune PaliGemma2 on Object Detection Dataset\n",
    "\n",
    "---\n",
    "\n",
    "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md)\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)\n",
    "\n",
    "PaliGemma 2 is built by combining the SigLIP-So400m vision encoder with the more recent and capable language models from the Gemma 2 family.\n",
    "\n",
    "![PaliGemma2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-1.png)\n",
    "\n",
    "The authors use a 3-stage training approach similar to the original PaliGemma. In stage 1, they combine the pretrained vision and language model components and train them jointly on a multimodal task mixture. In stage 2, they train the models at higher resolutions of 448px^2 and 896px^2. In stage 3, they fine-tune the models on the target transfer tasks.\n",
    "\n",
    "PaliGemma 2 models outperform the original PaliGemma at the same resolution and model size. Increasing the model size and resolution generally improves performance across a wide range of tasks, but the benefits differ depending on the task. Some tasks benefit more from increased resolution, while others benefit more from a larger language model.\n",
    "\n",
    "![PaliGemma2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/paligemma2-2.png)\n",
    "\n",
    "Notebook requires A100 with 40GB of VRAM to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBp3Czz3GBmc"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADTkh-2y_9Yv"
   },
   "source": [
    "### Configure your API keys\n",
    "\n",
    "To fine-tune PaliGemma2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
    "\n",
    "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
    "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
    "- In Colab, go to the left pane and click on `Secrets` (ðŸ”‘).\n",
    "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
    "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wyojKiG_hX9"
   },
   "source": [
    "### Select the runtime\n",
    "\n",
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `T4 GPU`, and then click `Save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_8BLW6R_x-z",
    "outputId": "678f6080-89ea-41f7-eecb-a592d1a03d82"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMlw3ru1YvLg"
   },
   "source": [
    "### Download dataset from Roboflow Universe\n",
    "\n",
    "To fine-tune PaliGemma2, prepare your dataset in JSONL format. You can use Roboflow to easily convert any dataset into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtvz4QZ9YuG8",
    "outputId": "8edb179f-f6a1-421d-f50a-2ee61724415a"
   },
   "outputs": [],
   "source": [
    "#!pip install -q peft bitsandbytes transformers==4.47.0 tf-keras\n",
    "!rsync -a --progress /data/lmbraid19/argusm/datasets/indoorCVPR_09.tar /tmp/ && mkdir -p /tmp/indoorCVPR && tar -xf /tmp/indoorCVPR_09.tar -C /tmp/indoorCVPR\n",
    "!rsync -a --progress /data/lmbraid19/argusm/datasets/clevr-act-7-depth /tmp/\n",
    "!file /tmp/indoorCVPR\n",
    "!file /tmp/clevr-act-7-depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm7ElnTKvBVa"
   },
   "source": [
    "**NOTE:** Let's read the first few lines of the annotation file and examine the dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLhSenP5AtQe",
    "outputId": "f6a6b7f0-6360-4eaf-8abf-bd40462af58a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dataset neither file nor dir: /tmp/clevr-act-7-depth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m randomize_background = RandomizeBackgrounds(p=\u001b[32m0.2\u001b[39m, background_images=bg_image_dataset)\n\u001b[32m     19\u001b[39m augment_depth = DepthAugmentation(depth_range=(\u001b[32m25\u001b[39m, \u001b[32m100\u001b[39m), max_delta_depth=\u001b[32m35\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train_dataset = \u001b[43mH5Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_rgbds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandomize_background\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment_image_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomplexify_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                          \u001b[49m\u001b[43maugment_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdataset_location:\u001b[39m\u001b[33m\"\u001b[39m, dataset_location,\u001b[33m\"\u001b[39m\u001b[33msamples:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/dlclarge2/zhangj-zhangj-CFM/cVLA/cvla/data_loader_h5.py:35\u001b[39m, in \u001b[36mH5Dataset.__init__\u001b[39m\u001b[34m(self, h5_file_or_dir, return_depth, augment_depth, depth_to_color, augment_rgbds, augment_rgb, augment_text, return_only_prefix, action_encoder, limit_samples, augment_rgb_forced, return_robot_pose)\u001b[39m\n\u001b[32m     33\u001b[39m     h5_file_path = h5_file_or_dir\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdataset neither file nor dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh5_file_or_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.h5_file = h5py.File(h5_file_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.return_depth = return_depth\n",
      "\u001b[31mValueError\u001b[39m: dataset neither file nor dir: /tmp/clevr-act-7-depth"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from data_loader_h5 import H5Dataset\n",
    "from data_loader_jsonl import JSONLDataset\n",
    "from data_augmentations import augment_image_rgb, RandomizeBackgrounds\n",
    "from data_augmentations import complexify_text, DepthAugmentation\n",
    "from data_loader_images import ImageFolderDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "model_location = Path(\"/data/lmbraid19/argusm/models\")\n",
    "dataset_location = Path(\"/tmp/clevr-act-7-depth\")\n",
    "\n",
    "run_name = \"_depthaug2\"\n",
    "bg_image_dataset = ImageFolderDataset(\"/tmp/indoorCVPR/Images\", transform=transforms.RandomResizedCrop((448,448)))\n",
    "randomize_background = RandomizeBackgrounds(p=0.2, background_images=bg_image_dataset)\n",
    "augment_depth = DepthAugmentation(depth_range=(25, 100), max_delta_depth=35)\n",
    "train_dataset = H5Dataset(dataset_location, augment_rgbds=randomize_background, augment_rgb=augment_image_rgb, augment_text=complexify_text,\n",
    "                          augment_depth=augment_depth, return_depth=True)\n",
    "\n",
    "print(\"dataset_location:\", dataset_location,\"samples:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McU9159EvkeA"
   },
   "source": [
    "### Set up and test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "19pQQjIixL-T",
    "outputId": "c59a8054-4f92-4cb0-b1cb-6a7b89e6e4b6"
   },
   "outputs": [],
   "source": [
    "from utils_vis import render_example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 3*2\n",
    "html_imgs = \"\"\n",
    "for i in range(num_samples):\n",
    "    image, sample = train_dataset[i]\n",
    "    prefix = sample[\"prefix\"]\n",
    "    html_imgs += render_example(image[0], label=sample[\"suffix\"], text=prefix, camera=sample[\"camera\"])\n",
    "    html_imgs += render_example(image[1], label=sample[\"suffix\"], text=prefix, camera=sample[\"camera\"])\n",
    "\n",
    "plot_images = True\n",
    "if plot_images:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(html_imgs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZvYNxYbBtE3"
   },
   "source": [
    "### Load PaliGemma2 model\n",
    "\n",
    "**NOTE:** PaliGemma2 offers 9 pre-trained models with sizes of `3B`, `10B`, and `28B` parameters, and resolutions of `224`, `448`, and `896` pixels. In this tutorial, I'll be using the [`google/paligemma2-3b-pt-448`](https://huggingface.co/google/paligemma2-3b-pt-448) checkpoint. Resolution has a key impact on the mAP of the trained model, and it seems that `448` offers the most optimal balance between performance and compute resources required to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "a5cd856a1fbf413a9aa4edff7cf3e636",
      "23be9405ad7f4cd8817db36f2f3834dd",
      "2e2e11bda8024f97868d45598bff6dc8",
      "11a966b75e124f369844afda5db25ab6",
      "37650cf71172405392ab843fd82360c3",
      "4fc0f4ab93ff4fddb482a8d6820b1253",
      "cda1f28093f04b1ca2e3689d1e314e10",
      "972c99a291d24b9f96ea09df63d5cf4b",
      "83aa606372b848659e13d683fa2fe09e",
      "3c26b58a776346f2920f810d99b21545",
      "fc9c7ae18f7341598a741ee8811896ff",
      "632c3fbb8739429b875687049eb1cf61",
      "7aeec69c06b049738c1fae480103cb1d",
      "c4e5a6e67ec3443fb73bfa86a27df704",
      "9355cb34cea948909504c1144169b693",
      "7a294c9159804eb388a2d7fe67f2920e",
      "7877bed5b8144f78a5a3f8b33741a82b",
      "dcb6f37a97664ed8ad2a699956bd772b",
      "ee2fa3db801344ba8564f700ca5ee2c5",
      "d5b373e9c1fc4d078e5512764235c6ba"
     ]
    },
    "id": "VfCgxIp3EjmC",
    "outputId": "db59a5d0-440f-4ca3-c5a3-194b9e22f7bd"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2f877f70e9b241f08717bba2b7616198",
      "9393bb98f83e4bf1bd8becdfdfb41f12",
      "41a50a96ca224e0aa12ef142d317e762",
      "cfcdcf908ff04fa19159c98b08c851ce",
      "9165a3ed1f2d4671b089dd00cae0fdbb",
      "6873590d9ac9436facef6cb182e1c077",
      "7ef24d8b5077400b9e005b373ab8d4f1",
      "df5fb754518547b480d95ce715bcdd8b",
      "c3eddb53526e4694bbf5ee3636e1b806",
      "eaf748f0d17148eba7e6351d107c1a50",
      "625fb0f94f5546e0a643192c2bd6c40f",
      "45db0bb8841f48e38c8eb8d28c1e72b8",
      "254a4ab182234215a7aeb6873f6c62af",
      "b7c2a5216879471c982406086571af7e",
      "9db3a6d6cd9548878d0998150f897ff1",
      "3226844ff08a46dcbc982432456d496b",
      "94f8f306b995471291fc610c4f170b26",
      "b91043be62ef4965852d9dcf5faf8edc",
      "141bc95f2dcc485cb9d23f0f01d28f24",
      "a770c3f09c204bf7b879a0d72256977d",
      "545d3d664c8e49cab3270012be39a604",
      "eeb274cb895f416bb85211d8ed07c005",
      "a3be6c18dd6440e48e80a9dc70bf9c9a",
      "f6619c93a7274805bb3005b0898d5f5f",
      "ef7ffed4ac344f7aaf1f39417284f4e3",
      "eab62ec555b146f4802630f6dd38afc6",
      "3dd89974f8b74268bf295119037ab0b8",
      "b18537a50e8f4e4d9829735ea2a98596",
      "d7f77f6a75f9441b8b1231b2eec3123a",
      "9f36c016c2a24d398e84c62964c47277",
      "dce11ec58ce84d9ea0ae3ac26519e796",
      "1a33f188e7ce499ebcb96e7e1617efe4",
      "be391a3e32d04aea837517041123cece",
      "4dd7115fec054f4790524d469bb263f7",
      "b6a7688a29f74e18b9c10ca185585457",
      "ed85f01229c146a48ae86370c36fb588",
      "2e5cf178fc03434faad8539094ddce4a",
      "ae8a2d84dd28458ba68bc94553337064",
      "20f03fcb438749f5a95cd3a72246fafd",
      "6f9622e6bdb1478e99ddddd1e2257f01",
      "f6e321f3c19f41688f3b2fd84243e851",
      "52d311c2b3af4979b433a392e77b5b21",
      "bddb4ea70dab445badbd5a4b2ef66ed4",
      "e9eb6315ff8a42f4911a3cee35e8320b"
     ]
    },
    "id": "ntXj4A3SyEAa",
    "outputId": "898b4343-0089-4ee6-8a54-d1f76d39b59f"
   },
   "outputs": [],
   "source": [
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "print(\"cuda visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "devices_good = sorted((int(x) for x in os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")))\n",
    "DEVICE = torch.device('cuda')\n",
    "print(DEVICE)\n",
    "print('Using device:', DEVICE)\n",
    "print(\"Good devices\", devices_good)\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE, device_map=\"auto\", attn_implementation='eager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def augment_suffix(suffix):\n",
    "    parts = suffix.split(' ; ')\n",
    "    random.shuffle(parts)\n",
    "    return ' ; '.join(parts)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=list(images),\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE).to(DEVICE)\n",
    "    print(\"prefixes\", prefixes)\n",
    "    return inputs\n",
    "\n",
    "batch = [train_dataset[i] for i in range(3)]\n",
    "inputs = collate_fn(batch)\n",
    "for x in inputs:\n",
    "    print(x, inputs[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    prefixes = [\"<image><image>\" + label[\"prefix\"] for label in labels]\n",
    "    suffixes = [augment_suffix(label[\"suffix\"]) for label in labels]\n",
    "    images_flat = [img for img_list_x in images for img in img_list_x]\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images_flat,\n",
    "        return_tensors=\"pt\",\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\"\n",
    "    ).to(TORCH_DTYPE)\n",
    "    return inputs\n",
    "\n",
    "batch = [train_dataset[i] for i in range(3)]\n",
    "inputs = collate_fn(batch)\n",
    "for x in inputs:\n",
    "    print(x, inputs[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, label_tokens = eval_pred  # Extract predictions and labels\n",
    "    if isinstance(predictions, tuple):  # Some models return tuples\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Convert to token indices if necessary (e.g., for text generation models)\n",
    "    pred_tokens = np.argmax(predictions, axis=-1)  # Assuming logits, take argmax\n",
    "\n",
    "    pred_texts = processor.tokenizer.batch_decode(pred_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "    label_text = processor.tokenizer.batch_decode(label_tokens[:,-SEQLEN-1:], skip_special_tokens=True)\n",
    "\n",
    "    print(pred_tokens[:,-SEQLEN-1:])\n",
    "    print(label_tokens[:,-SEQLEN-1:])\n",
    "    print(label_text)\n",
    "    print(pred_texts)\n",
    "    print()\n",
    "    return {\"accuracy\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with JAX settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        if \"self_attn\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "TRAIN_EXAMPLES = len(train_dataset)\n",
    "BATCH_SIZE = 32\n",
    "BATCH_SIZE_DEV = 2 # on l40 was 8\n",
    "GRAD_ACCUM = int(round(BATCH_SIZE / BATCH_SIZE_DEV))\n",
    "TRAIN_STEPS = (TRAIN_EXAMPLES // BATCH_SIZE)\n",
    "SEQLEN = 12\n",
    "SAVE_STEPS = int(TRAIN_STEPS / 15)\n",
    "SAVE_LIMIT = 5\n",
    "\n",
    "run_name = \"_depthaug\"\n",
    "save_path = model_location / (str(Path(dataset_location).stem) + run_name)\n",
    "print(\"save_path\", save_path)\n",
    "print(\"TRAIN_STEPS\",TRAIN_STEPS)\n",
    "print(\"GRAD_ACCUM\", GRAD_ACCUM)\n",
    "\n",
    "args_jax = Seq2SeqTrainingArguments(\n",
    "    max_steps=TRAIN_STEPS,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE_DEV,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=3e-5,  # 1e-5, 2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=.05,\n",
    "    generation_max_length=SEQLEN,\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_LIMIT,\n",
    "    output_dir=save_path,\n",
    "    bf16=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=4,\n",
    "    #dataloader_prefetch_factor=2,\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=4,\n",
    "    #per_device_eval_batch_size=BATCH_SIZE_DEV,\n",
    "    #eval_accumulation_steps=GRAD_ACCUM\n",
    ")\n",
    "#gradient_checkpointing=True,\n",
    "#weight_decay=3e-7,\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=train_dataset_small,\n",
    "    data_collator=collate_fn,\n",
    "    args=args_jax,\n",
    "    #compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
